{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823885f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import copy\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "maze_name = 'maze8n_5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9051e160",
   "metadata": {},
   "source": [
    "- 0 - 左\n",
    "- 1 - 向上\n",
    "- 2 - 右\n",
    "- 3 - 向下\n",
    "- 每次移动都会花费老鼠 -0.04 分\n",
    "- 奶酪，给予 1.0 分\n",
    "- 封锁的单元格-0.75 分，动作不会被执行\n",
    "- 迷宫边界之外的行为：-0.8 分，动作不会被执行\n",
    "- 已经访问过的单元格，-0.25 分\n",
    "- 总奖励低于负阈值：(-0.5 * maze.size)，lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e61dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import heapq\n",
    "\n",
    "def generate_gridworld(n, prob):\n",
    "    \"\"\"\n",
    "    生成大小为n x n的随机Gridworld，其中prob控制1出现的频率。\n",
    "    返回值是一个包含n个列表的列表，每个列表包含n个随机数（0或1）。\n",
    "    \"\"\"\n",
    "    gridworld = [[int(random.random() < prob) for j in range(n)] for i in range(n)]\n",
    "    return gridworld\n",
    "\n",
    "# def find_path(gridworld, start, end):\n",
    "#     \"\"\"\n",
    "#     使用深度优先搜索算法查找从start到end的路径。\n",
    "#     如果找到了一条路径，则返回True，否则返回False。\n",
    "#     \"\"\"\n",
    "#     visited = set()\n",
    "#     stack = [start]\n",
    "#     while stack:\n",
    "#         current = stack.pop()\n",
    "#         if current == end:\n",
    "#             return True\n",
    "#         if current in visited:\n",
    "#             continue\n",
    "#         visited.add(current)\n",
    "#         x, y = current\n",
    "#         neighbors = [(x+1, y), (x-1, y), (x, y+1), (x, y-1)]\n",
    "#         for neighbor in neighbors:\n",
    "#             i, j = neighbor\n",
    "#             if i < 0 or i >= len(gridworld) or j < 0 or j >= len(gridworld[0]):\n",
    "#                 continue\n",
    "#             if gridworld[i][j] == 0:\n",
    "#                 continue\n",
    "#             stack.append((i, j))\n",
    "#     return False\n",
    "\n",
    "def find_path(maze, start, end):\n",
    "    \"\"\"\n",
    "    使用 A* 算法搜索迷宫最优路径\n",
    "    :param maze: 二维迷宫数组，0 表示障碍，1 表示可通行\n",
    "    :param start: 起点坐标 (row, col)\n",
    "    :param end: 终点坐标 (row, col)\n",
    "    :return: 返回最优路径\n",
    "    \"\"\"\n",
    "    ROW, COL = len(maze), len(maze[0])\n",
    "    pq = []  # 使用优先队列存储搜索节点\n",
    "    heapq.heappush(pq, (0, start, [start]))\n",
    "    visited = set()  # 使用 set 存储已访问的节点\n",
    "    while pq:\n",
    "        f, (row, col), path = heapq.heappop(pq)\n",
    "        if (row, col) in visited:\n",
    "            continue\n",
    "        visited.add((row, col))\n",
    "        if (row, col) == end:\n",
    "            return path\n",
    "        for (r, c) in [(row-1, col), (row+1, col), (row, col-1), (row, col+1)]:\n",
    "            if 0 <= r < ROW and 0 <= c < COL and maze[r][c] == 1 and (r, c) not in visited:\n",
    "                g = len(path)  # 当前节点到起点的距离\n",
    "                h = abs(r-end[0]) + abs(c-end[1])  # 当前节点到终点的曼哈顿距离\n",
    "                f = g + h\n",
    "                heapq.heappush(pq, (f, (r, c), path + [(r, c)]))\n",
    "    return False\n",
    "\n",
    "\n",
    "def generate_solvable_gridworld(n, prob=0.6):\n",
    "    \"\"\"\n",
    "    生成一个可解的大小为n x n的Gridworld。\n",
    "    返回值是一个元组，包含生成的Gridworld、起点和终点。\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        gridworld = generate_gridworld(n, prob=prob)\n",
    "        # start = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "        # end = (random.randint(0, n-1), random.randint(0, n-1))\n",
    "        start = (0, 0)\n",
    "        end = (n-1, n-1)\n",
    "        \n",
    "        optimal_path = find_path(gridworld, start, end)\n",
    "        if gridworld[start[0]][start[1]] == 1 and gridworld[end[0]][end[1]] == 1 and start != end and optimal_path is not False:\n",
    "            return gridworld, start, end, optimal_path\n",
    "\n",
    "# 示例代码\n",
    "# gridworld, start, end, optimal_path = generate_solvable_gridworld(8)\n",
    "# gridworld = np.array(gridworld)*1.0\n",
    "# print(gridworld)\n",
    "# print(\"start:\", start)\n",
    "# print(\"end:\", end)\n",
    "\n",
    "# train_set = {'gridworld':[],\n",
    "#             'start':[],\n",
    "#             'end':[]}\n",
    "\n",
    "# test_set = {'gridworld':[],\n",
    "#             'start':[],\n",
    "#             'end':[]}\n",
    "\n",
    "# for i in range(6400):\n",
    "#     gridworld, start, end = generate_solvable_gridworld(8)\n",
    "#     train_set['gridworld'].append(gridworld)\n",
    "#     train_set['start'].append(start)\n",
    "#     train_set['end'].append(end)\n",
    "\n",
    "# # 将字典保存到文件中\n",
    "# with open(\"gridworld3x3_train_dict.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09621d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "end_mark = 1.5\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a9f1b",
   "metadata": {},
   "source": [
    "## Q-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431c3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0), max_Tstep=800):\n",
    "        # 允许的最大步数\n",
    "        self.max_Tstep = max_Tstep\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        # 初始化迷宫，老鼠可以从任意位置开始，默认为左上角\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        # 终点始终在右下角\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        # 初始化空格list，maze为1表示空格，为0表示墙体\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        # 将目标格移出空格list\n",
    "        self.free_cells.remove(self.target)\n",
    "        # 检查左上和右下是否为空\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        # 放置老鼠并初始化参数\n",
    "        state, info = self.reset(rat)\n",
    "    \n",
    "        # return state, info\n",
    "\n",
    "    def reset(self, rat=(0, 0)):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.maze[self.target[0], self.target[1]] = end_mark\n",
    "        # 初始状态\n",
    "        self.state = (row, col, 'start')\n",
    "        # 设置最低奖励阈值\n",
    "        self.min_reward = -100 # -0.5 * self.maze.size\n",
    "        # 初始化总奖励\n",
    "        self.total_reward = 0\n",
    "        self.visited = list()\n",
    "        self.total_Tstep = 0\n",
    "        \n",
    "        return self.observe(), self.game_status()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        '''\n",
    "            input: action [0, 1, 2, 3] [L, U, R, D]\n",
    "        '''\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "        \n",
    "        # 如果老鼠访问的是空格，则记录\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.append((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        # 获取所有可能执行的动作\n",
    "        valid_actions = self.valid_actions()\n",
    "        # print('valid_actions', valid_actions)\n",
    "        \n",
    "        # 如果没有可以执行的动作（被围住了），则状态为 blocked，位置不变\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "            print('blocked')\n",
    "        # 如果需要执行的动作在可执行动作列表中，那么状态为有效，并相应执行动作\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        # 如果需要执行的动作不在可执行动作列表中（撞墙），位置不变\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            nmode = 'invalid'\n",
    "            \n",
    "        self.total_Tstep += 1  # 每次执行动作+1\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        reward = 0\n",
    "        rl = 0\n",
    "        rg = 0\n",
    "        \n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            rl = 50  # 奶酪，给予 1.0 分\n",
    "        # elif mode == 'blocked':\n",
    "        #     rl = self.min_reward - 1\n",
    "        # elif (rat_row, rat_col) in self.visited:\n",
    "        #     rl = -0.25  # 访问已经访问过的单元格，-0.25 分\n",
    "        elif mode == 'invalid':\n",
    "            rl = -10  # 撞墙-0.75 分，动作不会被执行\n",
    "        elif mode == 'valid':\n",
    "            rl = -1  # 每次移动都会花费老鼠 -0.04 分\n",
    "        \n",
    "        # rg = self._maze.shape[0]/(abs(self.state[0]-self.target[0]) + abs(self.state[1]-self.target[1]))\n",
    "        # print(rl, rg)\n",
    "        \n",
    "        reward = rl + rg\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "    \n",
    "    def step(self, action):\n",
    "        envstate, reward, status = self.act(action)\n",
    "        observation = envstate\n",
    "        done = self.is_game_done()\n",
    "        info = status\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def observe(self):\n",
    "        # canvas = self.draw_env()\n",
    "        canvas = self.get_observation()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        canvas[self.target[0], self.target[1]] = end_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_Tstep > self.max_Tstep or self.total_reward < self.min_reward:\n",
    "        # if self.total_reward < self.min_reward:\n",
    "        # if self.total_Tstep > self.max_Tstep:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "    \n",
    "    def is_game_done(self):\n",
    "        game_status = self.game_status()\n",
    "        \n",
    "        if game_status == 'not_over':\n",
    "            return False\n",
    "        elif game_status == 'win' or game_status == 'lose':\n",
    "            return True\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        # 默认验证当前位置\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = copy.deepcopy(self.action_space)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # 如果在第0行，则不能向上走；如果在最后一行，则不能向下走\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "        # 列-左右\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 如果不在最左列，而左边是墙，则不能向左；右边同理\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        # 上下同理\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 返回所有可能执行的动作\n",
    "        return actions\n",
    "    \n",
    "    def get_observation(self, size=3):\n",
    "        maze = self.draw_env()\n",
    "        row, col, _ = self.state\n",
    "        # 获取maze的行列数\n",
    "        ROWS = len(maze)\n",
    "        COLS = len(maze[0])\n",
    "\n",
    "        # 初始化结果二维数组\n",
    "        result = [[0 for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "        # 将以指定点为中心指定尺寸范围的观测值存入结果二维数组\n",
    "        for i in range(row-size//2, row+size//2+1):\n",
    "            for j in range(col-size//2, col+size//2+1):\n",
    "                if i < 0 or i >= ROWS or j < 0 or j >= COLS:\n",
    "                    # 如果超出边界，则填充为1\n",
    "                    result[i-row+size//2][j-col+size//2] = 0.0\n",
    "                else:\n",
    "                    result[i-row+size//2][j-col+size//2] = maze[i][j]\n",
    "\n",
    "        # 返回结果二维数组\n",
    "        result = np.array(result)\n",
    "        result[size//2][size//2] = 0.5\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c222ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show(qmaze):\n",
    "#     plt.grid('on')\n",
    "#     nrows, ncols = qmaze.maze.shape\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "#     ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "#     ax.set_xticklabels([])\n",
    "#     ax.set_yticklabels([])\n",
    "#     canvas = np.copy(qmaze.maze)\n",
    "#     for row,col in qmaze.visited:\n",
    "#         canvas[row,col] = 0.6\n",
    "#     rat_row, rat_col, _ = qmaze.state\n",
    "#     canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "#     canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "#     img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de9d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze_size = 8\n",
    "# obstical_prob = 0.2\n",
    "# gridworld, start, end, optimal_path = generate_solvable_gridworld(maze_size, 1-obstical_prob)\n",
    "# maze = np.array(gridworld)*1.0\n",
    "# print(maze)\n",
    "# print(\"start:\", start)\n",
    "# print(\"end:\", end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ce0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze = np.array([\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "#     [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "#     [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "#     [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "# ])\n",
    "# maze =  np.array([\n",
    "#     [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 1.],\n",
    "#     [ 1.,  1.,  1.,  0.,  0.,  1.,  0., 0.],\n",
    "#     [ 1.,  0.,  1.,  1.,  1.,  1.,  0., 0.],\n",
    "#     [ 1.,  1.,  1.,  1.,  0.,  1.,  1., 1.],\n",
    "#     [ 1.,  0.,  0.,  1.,  1.,  1.,  1., 1.],\n",
    "#     [ 1.,  0.,  1.,  1.,  1.,  1.,  1., 1.],\n",
    "#     [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 1.],\n",
    "#     [ 1.,  1.,  1.,  0.,  1.,  1.,  1., 1.],\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a96f2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('maze32_1.txt', maze)\n",
    "maze = np.loadtxt(maze_name+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7767b985-e2da-4e1b-9638-8dbfd77481a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1586f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal path length is: 15\n"
     ]
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "maze_size = maze.shape[0]\n",
    "optimal_path = find_path(maze, (0, 0), (maze_size-1, maze_size-1))\n",
    "qmaze.visited = optimal_path\n",
    "optimal_length = len(optimal_path)\n",
    "\n",
    "# show(qmaze)\n",
    "print('optimal path length is:', optimal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b57a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qmaze = Qmaze(maze)\n",
    "# canvas, reward, game_over = qmaze.act(RIGHT)\n",
    "# print(\"reward=\", reward)\n",
    "# show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc459b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4bb42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qmaze.act(RIGHT)  # move right\n",
    "# qmaze.act(DOWN)  # move right\n",
    "# qmaze.act(DOWN)  # move up\n",
    "# qmaze.act(DOWN)  # move up\n",
    "# qmaze.act(RIGHT)  # move up\n",
    "# qmaze.act(RIGHT)  # move up\n",
    "# qmaze.act(RIGHT)  # move up\n",
    "# qmaze.act(DOWN)  # move right\n",
    "# qmaze.act(DOWN)  # move up\n",
    "# qmaze.act(DOWN)  # move up\n",
    "# qmaze.act(DOWN)  # move up\n",
    "# qmaze.act(RIGHT)  # move up\n",
    "# qmaze.act(RIGHT)  # move up\n",
    "# # show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e708d8d7-ea1e-44ea-a062-fc99e5d41c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. , 1. , 0.5, 0. , 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmaze.observe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044ca78",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb60c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb2b0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, observation_size, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(observation_size, maze_size*maze_size*2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(maze_size*maze_size*2, maze_size*maze_size*2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(maze_size*maze_size*2, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ca14e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# set up matplotliba\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def plot_rewards(episode_rewards, show_result=False, zero_point=None, ylabel='Rewards'):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    \n",
    "    if zero_point is None:\n",
    "        zero_point = (maze_size*maze_size*0.5)\n",
    "    \n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99)-zero_point, means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e398e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动作选取\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    \n",
    "    # 随着进行，eps_threshold逐渐降低\n",
    "    # eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    #     math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    eps_threshold = 0.1\n",
    "    steps_done += 1\n",
    "    \n",
    "    # 常规情况选择价值最高的动作\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    \n",
    "    # 当随机值超过阈值时，随机选取 - exploration\n",
    "    else:\n",
    "        # 探索时只探索可能的动作，增加探索效率？\n",
    "        return torch.tensor([[random.choice(env.valid_actions())]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a8a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # 离线学习，从记忆池中抽取回忆\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # print(transitions)\n",
    "    \n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    \n",
    "    # 将([a, 1], [b, 2], [c, 3])转化为([a, b, c], [1, 2, 3])，一个zip的trick\n",
    "    # 然后将他们分别放到tuples with names里（'state', 'action', 'next_state', and 'reward'）\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    # 计算非最终状态的掩码，并将批处理元素连接起来\n",
    "    # (最终状态是指模拟结束后的状态)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    # 模型计算Q价值，我们根据价值选择动作\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    # 当前奖励+下一个状态的奖励，更新Q. 如果下一个状态为最终状态，则仅有当前奖励\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  \n",
    "    # print(expected_state_action_values)\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "# optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0a5c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b4a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = []\n",
    "# es = []\n",
    "# EPS_DECAY = 10\n",
    "# for i in range(1000):\n",
    "#     sample = random.random()\n",
    "#     eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "#             math.exp(-1. * i / EPS_DECAY)\n",
    "#     # eps_threshold = 1.0 / (i + 1)\n",
    "#     ss.append(sample)\n",
    "#     es.append(eps_threshold)\n",
    "\n",
    "# exploit = 0\n",
    "# for i in range(1000):\n",
    "#     if ss[i] > es[i]:\n",
    "#         exploit += 1\n",
    "# plt.plot(ss)\n",
    "# plt.plot(es)\n",
    "# print(exploit, exploit/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68fc44e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "414e85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def evaluation():\n",
    "    global optimal_length\n",
    "    win = 0\n",
    "    episode_rewards_eval = []\n",
    "   \n",
    "    env = Qmaze(maze, max_Tstep=optimal_length)\n",
    "\n",
    "    for j in range(1):\n",
    "    \n",
    "        # Initialize the environment and get it's state\n",
    "        state, info = env.reset()\n",
    "        # Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)  # 选择一个动作\n",
    "            # random.choice(env.valid_actions())\n",
    "            observation, reward, done, info = env.step(action.item())  # 执行动作，返回{下一个观察值、奖励、是否结束、是否提前终止}\n",
    "            # reward = torch.tensor([reward], device=device)\n",
    "            # print(int(action[0][0]))\n",
    "            # print(observation, reward, done, info)\n",
    "            # print()\n",
    "            if done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device)  # 如果没有终止则继续记录下一个状态\n",
    "\n",
    "            # Store the transition in memory\n",
    "            # memory.push(state, action, next_state, reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "        episode_rewards_eval.append(env.total_reward)\n",
    "        if info == 'win':\n",
    "            win += 1\n",
    "    \n",
    "    \n",
    "    win_rate = win / 1\n",
    "    \n",
    "    # show(env)\n",
    "    # print(env.visited)\n",
    "    # print(env.state)\n",
    "    # print(env.total_reward)\n",
    "    \n",
    "    return episode_rewards_eval, win_rate\n",
    "\n",
    "# win_rate = evaluation(10)\n",
    "# print(win_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ed47b215-d121-4d80-b5f9-ad7187321ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'grid'\n",
    "\n",
    "now = time.strftime(\"%m-%d_%H-%M-%S\", time.localtime())\n",
    "folder_name = f\"runs/{ENV_NAME}/\" + now\n",
    "os.makedirs('runs/', exist_ok=True)\n",
    "os.makedirs(f'runs/{ENV_NAME}/', exist_ok=True)\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3e880-1cce-4bfe-8e59-9ac822b68020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV_NAME = 'grid'\n",
    "# now = time.strftime(\"%m-%d_%H-%M-%S\", time.localtime())\n",
    "# folder_name = f\"runs/{ENV_NAME}/\" + now\n",
    "# os.makedirs('runs/', exist_ok=True)\n",
    "# os.makedirs(f'runs/{ENV_NAME}/', exist_ok=True)\n",
    "# os.makedirs(folder_name, exist_ok=True)\n",
    "#\n",
    "# # tensorboard\n",
    "# writer = SummaryWriter(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4dc8c1ee-1406-4548-803b-b812909f8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "history['win'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b0319656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "# BATCH_SIZE是指从重放缓冲区采样的转换数\n",
    "# GAMMA是上一节中提到的折扣系数\n",
    "# EPS_START是EPSILON的起始值\n",
    "# EPS_END是epsilon的最终值\n",
    "# EPS_DECAY 控制epsilon的指数衰减率，越高意味着衰减越慢\n",
    "# TAU是目标网络的更新率\n",
    "# LR是AdamW优化器的学习率\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-3\n",
    "num_episodes = 500\n",
    "steps_done = 0\n",
    "# sync_target_net_freq = 1e4\n",
    "\n",
    "# 初始化环境\n",
    "env = Qmaze(maze, max_Tstep=int(maze.size*0.5/0.04))\n",
    "# 重置环境获取信息\n",
    "state, info = env.reset()\n",
    "\n",
    "n_observations = state.size\n",
    "state = torch.Tensor(state).to(device)\n",
    "\n",
    "policy_net = DQN(n_observations, num_actions).to(device)\n",
    "target_net = DQN(n_observations, num_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(int(maze.size*0.5/0.04)*2)\n",
    "\n",
    "episode_rewards = []\n",
    "episode_step = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    # Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "    done = False\n",
    "    for t in count():\n",
    "        action = select_action(state)  # 选择一个动作\n",
    "        # random.choice(env.valid_actions())\n",
    "        observation, reward, done, _ = env.step(action.item())  # 执行动作，返回{下一个观察值、奖励、是否结束、是否提前终止}\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device)  # 如果没有终止则继续记录下一个状态\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        \n",
    "        # if (steps_done % sync_target_net_freq) == 0:\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "            # target_net.load_state_dict(policy_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(env.total_reward)\n",
    "    episode_step.append(t)\n",
    "    \n",
    "    history['episode_rewards'] = episode_rewards\n",
    "    history['episode_step'] = episode_step\n",
    "    # 将字典保存成 txt 文件\n",
    "    with open(folder_name+'/history.txt', 'w') as f:\n",
    "        for key, value in history.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "    \n",
    "    plot_rewards(episode_step, False, -15, 'Steps')\n",
    "    \n",
    "    episode_rewards_eval, win_rate = evaluation()\n",
    "\n",
    "    history['win'].append(win_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "fafeec2a-8b4f-4b36-96df-9f0cda3f8ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_rewards(episode_rewards, True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c2e0be63-2f2b-460b-a7f2-50fe49fc25ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot_rewards(episode_step, True, -15, 'Steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6b018ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7f35d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the environment and get it's state\n",
    "# state, info = env.reset()\n",
    "# # Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
    "# state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = policy_net(state).max(1)[1].view(1, 1)  # 选择一个动作\n",
    "#     # random.choice(env.valid_actions())\n",
    "#     observation, reward, done, _ = env.step(action.item())  # 执行动作，返回{下一个观察值、奖励、是否结束、是否提前终止}\n",
    "#     # reward = torch.tensor([reward], device=device)\n",
    "\n",
    "#     if done:\n",
    "#         next_state = None\n",
    "#     else:\n",
    "#         next_state = torch.tensor(observation, dtype=torch.float32, device=device)  # 如果没有终止则继续记录下一个状态\n",
    "\n",
    "#     # Store the transition in memory\n",
    "#     # memory.push(state, action, next_state, reward)\n",
    "\n",
    "#     # Move to the next state\n",
    "#     state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d1da02c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (5, 3), (6, 3), (6, 4), (6, 5), (7, 5), (7, 6)]\n",
      "37\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show(env)\n",
    "# print(env.visited)\n",
    "# print(env.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6506686-d944-4ce3-8262-bfd4dc13f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "history['test_route'] = env.visited\n",
    "history['test_total_reward'] = env.total_reward\n",
    "\n",
    "# 将字典保存成 txt 文件\n",
    "with open(folder_name+'/history.txt', 'w') as f:\n",
    "    for key, value in history.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "\n",
    "torch.save(policy_net.state_dict(), folder_name+'/my_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e263633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.total_Tstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed93933a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook GridWorld_report_v2_part_ob.ipynb to python\n",
      "[NbConvertApp] Writing 37256 bytes to GridWorld_report_v2_part_ob.py\n"
     ]
    }
   ],
   "source": [
    "# ! jupyter nbconvert --to python GridWorld_report_v2_part_ob.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90939b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f28fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35765a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a98b6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play_game(model, qmaze, rat_cell):\n",
    "#     qmaze.reset(rat_cell)\n",
    "#     envstate = qmaze.observe()\n",
    "#     while True:\n",
    "#         prev_envstate = envstate\n",
    "#         # get next action\n",
    "#         q = model.predict(prev_envstate)\n",
    "#         action = np.argmax(q[0])\n",
    "\n",
    "#         # apply action, get rewards and new state\n",
    "#         envstate, reward, game_status = qmaze.act(action)\n",
    "#         if game_status == 'win':\n",
    "#             return True\n",
    "#         elif game_status == 'lose':\n",
    "#             return False\n",
    "        \n",
    "# def completion_check(model, qmaze):\n",
    "#     for cell in qmaze.free_cells:\n",
    "#         if not qmaze.valid_actions(cell):\n",
    "#             return False\n",
    "#         if not play_game(model, qmaze, cell):\n",
    "#             return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9cfdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Experience(object):\n",
    "#     def __init__(self, model, max_memory=100, discount=0.95):\n",
    "#         self.model = model\n",
    "#         self.max_memory = max_memory\n",
    "#         self.discount = discount\n",
    "#         self.memory = list()\n",
    "#         self.num_actions = num_actions\n",
    "\n",
    "#     def remember(self, episode):\n",
    "#         # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "#         # memory[i] = episode\n",
    "#         # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "#         self.memory.append(episode)\n",
    "#         if len(self.memory) > self.max_memory:\n",
    "#             del self.memory[0]\n",
    "\n",
    "#     def predict(self, envstate):\n",
    "#         envstate = torch.Tensor(envstate)\n",
    "#         return self.model(envstate)\n",
    "\n",
    "#     def get_data(self, data_size=10):\n",
    "#         env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "#         mem_size = len(self.memory)\n",
    "#         data_size = min(mem_size, data_size)\n",
    "#         inputs = torch.zeros((data_size, env_size))\n",
    "#         targets = torch.zeros((data_size, self.num_actions))\n",
    "#         for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "#             envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "#             inputs[i] = envstate\n",
    "#             # There should be no target values for actions not taken.\n",
    "#             targets[i] = self.predict(envstate)\n",
    "#             # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "#             Q_sa = torch.max(self.predict(envstate_next))\n",
    "#             print(targets, reward)\n",
    "#             if game_over:\n",
    "#                 targets[i, action] = reward\n",
    "#             else:\n",
    "#                 # reward + gamma * max_a' Q(s', a')\n",
    "#                 targets[i, action] = reward + self.discount * Q_sa\n",
    "#             print(targets, reward)\n",
    "#             return\n",
    "#         return inputs, targets\n",
    "    \n",
    "#     def get_data_tensor(self, data_size=10):\n",
    "#         env_size = self.memory[0][0].shape[1]  # envstate 1d size (1st element of episode)\n",
    "#         mem_size = len(self.memory)\n",
    "#         data_size = min(mem_size, data_size)\n",
    "#         inputs = torch.zeros((data_size, env_size))\n",
    "#         targets = torch.zeros((data_size, self.num_actions))\n",
    "\n",
    "#         sample_indices = np.random.choice(range(mem_size), data_size, replace=False)\n",
    "#         sampled_memory = [self.memory[j] for j in sample_indices]\n",
    "       \n",
    "#         envstates = torch.cat([m[0] for m in sampled_memory])\n",
    "#         actions = torch.LongTensor([m[1] for m in sampled_memory]).view(-1, 1)\n",
    "#         rewards = torch.FloatTensor([m[2] for m in sampled_memory]).view(-1, 1)\n",
    "#         envstates_next = torch.cat([m[3] for m in sampled_memory])\n",
    "#         game_over = torch.FloatTensor([m[4] for m in sampled_memory]).view(-1, 1)\n",
    "\n",
    "#         inputs.copy_(envstates)\n",
    "#         # There should be no target values for actions not taken.\n",
    "#         targets.copy_(self.predict(envstates).data)\n",
    "#         # print(torch.max(self.predict(envstates_next)))\n",
    "#         # print(self.predict(envstates_next).shape)\n",
    "#         # print(envstates_next.shape)\n",
    "#         Q_sa = torch.max(self.predict(envstates_next), dim=1, keepdim=True)[0]\n",
    "#         print('Q_sa', Q_sa)\n",
    "#         # print(targets.gather(1, actions).view(-1, 1))\n",
    "#         print(targets, rewards)\n",
    "#         targets.gather(1, actions).view(-1, 1).copy_(rewards)\n",
    "#         print(targets.gather(1, actions).view(-1, 1))\n",
    "#         print(targets, rewards)\n",
    "#         # return\n",
    "#         mask = (game_over == 0)\n",
    "#         masked_next_state_values = Q_sa * mask.float()\n",
    "#         targets.gather(1, actions).view(-1, 1).add_(masked_next_state_values)\n",
    "\n",
    "#         return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e9a0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a95fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(maze.size, num_actions)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c7a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5db972eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(maze)\n",
    "# qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4255177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is a small utility for printing readable time strings:\n",
    "# def format_time(seconds):\n",
    "#     if seconds < 400:\n",
    "#         s = float(seconds)\n",
    "#         return \"%.1f seconds\" % (s,)\n",
    "#     elif seconds < 4000:\n",
    "#         m = seconds / 60.0\n",
    "#         return \"%.2f minutes\" % (m,)\n",
    "#     else:\n",
    "#         h = seconds / 3600.0\n",
    "#         return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "397d0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global epsilon\n",
    "# n_epoch = 1\n",
    "# max_memory = 8*maze.size\n",
    "# data_size = 32\n",
    "# weights_file = \"\"\n",
    "# name = 'model'\n",
    "# start_time = datetime.datetime.now()\n",
    "\n",
    "# # If you want to continue training from a previous model,\n",
    "# # just supply the h5 file name to weights_file option\n",
    "# if weights_file:\n",
    "#     print(\"loading weights from file: %s\" % (weights_file,))\n",
    "#     model.load_weights(weights_file)\n",
    "\n",
    "# # Construct environment/game from numpy array: maze (see above)\n",
    "# qmaze = Qmaze(maze)\n",
    "\n",
    "# # Initialize experience replay object\n",
    "# experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "# win_history = []   # history of win/lose game\n",
    "# n_free_cells = len(qmaze.free_cells)\n",
    "# hsize = qmaze.maze.size//2   # history window size\n",
    "# win_rate = 0.0\n",
    "# imctr = 1\n",
    "\n",
    "# for epoch in range(n_epoch):\n",
    "#     loss = 0.0\n",
    "#     rat_cell = random.choice(qmaze.free_cells)\n",
    "#     qmaze.reset(rat_cell)\n",
    "#     game_over = False\n",
    "\n",
    "#     # get initial envstate (1d flattened canvas)\n",
    "#     envstate = qmaze.observe()  # (1, 64)\n",
    "#     envstate = torch.Tensor(envstate)\n",
    "\n",
    "#     n_episodes = 0\n",
    "#     while not game_over:\n",
    "#         valid_actions = qmaze.valid_actions()\n",
    "#         if not valid_actions: break\n",
    "#         prev_envstate = envstate  # 上一个时刻的状态\n",
    "        \n",
    "#         # Get next action\n",
    "#         if np.random.rand() < epsilon:\n",
    "#             action = random.choice(valid_actions)\n",
    "#         else:\n",
    "#             action = np.argmax(experience.predict(prev_envstate).cpu().detach().numpy())\n",
    "            \n",
    "#         # Apply action, get reward and new envstate\n",
    "#         envstate, reward, game_status = qmaze.act(action)\n",
    "#         envstate = torch.Tensor(envstate)\n",
    "        \n",
    "#         if game_status == 'win':\n",
    "#             win_history.append(1)\n",
    "#             game_over = True\n",
    "#         elif game_status == 'lose':\n",
    "#             win_history.append(0)\n",
    "#             game_over = True\n",
    "#         else:\n",
    "#             game_over = False\n",
    "\n",
    "#         # Store episode (experience)\n",
    "#         episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "#         experience.remember(episode)\n",
    "#         n_episodes += 1\n",
    "        \n",
    "#         # Train neural network model\n",
    "#         inputs, targets = experience.get_data(data_size=data_size)\n",
    "\n",
    "#         # break\n",
    "        \n",
    "# inputs, inputs.shape, targets, targets.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #         h = model.fit(\n",
    "# #             inputs,\n",
    "# #             targets,\n",
    "# #             epochs=8,\n",
    "# #             batch_size=16,\n",
    "# #             verbose=0,\n",
    "# #         )\n",
    "# #         loss = model.evaluate(inputs, targets, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f6dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(n_epoch):\n",
    "#     loss = 0.0\n",
    "#     rat_cell = random.choice(qmaze.free_cells)\n",
    "#     qmaze.reset(rat_cell)\n",
    "#     game_over = False\n",
    "\n",
    "#     # get initial envstate (1d flattened canvas)\n",
    "#     envstate = qmaze.observe()\n",
    "\n",
    "#     n_episodes = 0\n",
    "#     while not game_over:\n",
    "#         valid_actions = qmaze.valid_actions()\n",
    "#         if not valid_actions: break\n",
    "#         prev_envstate = envstate\n",
    "#         # Get next action\n",
    "#         if np.random.rand() < epsilon:\n",
    "#             action = random.choice(valid_actions)\n",
    "#         else:\n",
    "#             action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "#         # Apply action, get reward and new envstate\n",
    "#         envstate, reward, game_status = qmaze.act(action)\n",
    "#         if game_status == 'win':\n",
    "#             win_history.append(1)\n",
    "#             game_over = True\n",
    "#         elif game_status == 'lose':\n",
    "#             win_history.append(0)\n",
    "#             game_over = True\n",
    "#         else:\n",
    "#             game_over = False\n",
    "\n",
    "#         # Store episode (experience)\n",
    "#         episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "#         experience.remember(episode)\n",
    "#         n_episodes += 1\n",
    "\n",
    "#         # Train neural network model\n",
    "#         inputs, targets = experience.get_data(data_size=data_size)\n",
    "#         h = model.fit(\n",
    "#             inputs,\n",
    "#             targets,\n",
    "#             epochs=8,\n",
    "#             batch_size=16,\n",
    "#             verbose=0,\n",
    "#         )\n",
    "#         loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "#     if len(win_history) > hsize:\n",
    "#         win_rate = sum(win_history[-hsize:]) / hsize\n",
    "\n",
    "#     dt = datetime.datetime.now() - start_time\n",
    "#     t = format_time(dt.total_seconds())\n",
    "#     template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "#     print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "#     # we simply check if training has exhausted all free cells and if in all\n",
    "#     # cases the agent won\n",
    "#     if win_rate > 0.9 : epsilon = 0.05\n",
    "#     if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "#         print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "#         break\n",
    "\n",
    "# # Save trained model weights and architecture, this will be used by the visualization code\n",
    "# h5file = name + \".h5\"\n",
    "# json_file = name + \".json\"\n",
    "# model.save_weights(h5file, overwrite=True)\n",
    "# with open(json_file, \"w\") as outfile:\n",
    "#     json.dump(model.to_json(), outfile)\n",
    "# end_time = datetime.datetime.now()\n",
    "# dt = datetime.datetime.now() - start_time\n",
    "# seconds = dt.total_seconds()\n",
    "# t = format_time(seconds)\n",
    "# print('files: %s, %s' % (h5file, json_file))\n",
    "# print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
