{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP6629 001 RL Spring 2023\n",
    "# Yiran Pang\n",
    "# Copied and adapted from https://www.samyzaf.com/ML/rl/qmaze.html about Maze Definition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0 - 左\n",
    "- 1 - 向上\n",
    "- 2 - 右\n",
    "- 3 - 向下\n",
    "- 每次移动都会花费老鼠 -0.04 分\n",
    "- 奶酪，给予 1.0 分\n",
    "- 封锁的单元格-0.75 分，动作不会被执行\n",
    "- 已经访问过的单元格，-0.25 分\n",
    "- 总奖励低于负阈值：(-0.5 * maze.size)，lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        # 初始化迷宫，老鼠可以从任意位置开始，默认为左上角\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        # 终点始终在右下角\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        # 初始化空格list，maze为1表示空格，为0表示墙体\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        # 将目标格移出空格list\n",
    "        self.free_cells.remove(self.target)\n",
    "        # 检查左上和右下是否为空\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        # 放置老鼠并初始化参数\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        # 初始状态\n",
    "        self.state = (row, col, 'start')\n",
    "        # 设置最低奖励阈值\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        # 初始化总奖励\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        '''\n",
    "            input: action [0, 1, 2, 3] [L, U, R, D]\n",
    "        '''\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "        \n",
    "        # 如果老鼠访问的是空格，则记录\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        # 获取所有可能执行的动作\n",
    "        valid_actions = self.valid_actions()\n",
    "        # print('valid_actions', valid_actions)\n",
    "        \n",
    "        # 如果没有可以执行的动作（被围住了），则状态为 blocked，位置不变\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        # 如果需要执行的动作在可执行动作列表中，那么状态为有效，并相应执行动作\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        # 如果需要执行的动作不在可执行动作列表中（撞墙），位置不变\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            nmode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0  # 奶酪，给予 1.0 分\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        # if (rat_row, rat_col) in self.visited:\n",
    "        #     return -0.25  # 访问已经访问过的单元格，-0.25 分\n",
    "        if mode == 'invalid':\n",
    "            return -0.75  # 撞墙-0.75 分，动作不会被执行\n",
    "        if mode == 'valid':\n",
    "            return -0.04  # 每次移动都会花费老鼠 -0.04 分\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        # 默认验证当前位置\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # 如果在第0行，则不能向上走；如果在最后一行，则不能向下走\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "        # 列-左右\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 如果不在最左列，而左边是墙，则不能向左；右边同理\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        # 上下同理\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 返回所有可能执行的动作\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  1., 1.],\n",
    "    [ 1.,  1., 1.],\n",
    "    [ 1.,  1., 1.],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17bddbf7e80>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEHUlEQVR4nO3XsW0bWRRA0T8LJYIBGhAWcOhAwRYgFsAanKgDFqVOFGkKYAMLyIAKUEgFjMbJhhZlL6wlr/ecjPg/eHjgxcxMy7IM4Pz9ceoBgB8jVogQK0SIFSLEChFihYiLn7l8eXm5rFar95ol7+rqanz48OHUY5y1l5cXOzri6elpPD8/T987+6lYV6vVuL29/TVT/Ya+fPkyNpvNqcc4a/M829ER6/X61TOvwRAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoWIi7cuTNO0HWNsxxjj48eP4/Pnz+8+VNV+vx/zPJ96jLNmR//etCzLj1+eph+//D/08PAwNpvNqcc4a/M829ER6/V67Ha76XtnXoMhQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKERc/c/nm5mbsdrv3miVvnudTj3D2DofDeHx8PPUYZ+twOLx69mas0zRtxxjbMcb49OmTP+QR+/3eft5wOBzG169fTz1G0puxLstyN8a4G2OM9Xq9bDab954pa57nYT/H3d/fj+vr61OPkeSbFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBBx8daFaZq2Y4ztPz/30zT9/b4jpf05xng+9RBnzo6O++u1g2lZlv9ykN/aNE27ZVnWp57jnNnRccf24zUYIsQKEWL9te5OPUCAHR336n58s0KEJytEiBUixAoRYoUIsULENwMabjSrlY/fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04 -0.08\n",
      "-0.04 -0.12\n",
      "1.0 0.88 win\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17bddc49280>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEF0lEQVR4nO3ZsW1bSRRA0ZmFMkNgsqFyuwAWIDfDCpSqA+UCtg0WYDqXC1DmApTZCoW/yYYSJS9sfF77nIyYCR4eeMH/wbksywBO319rDwC8jVghQqwQIVaIECtEiBUizn7k8pzT/zxHfPjwYbx7927tMU7a4+OjHR3x9evX8fDwMJ87+6FYOe729nZcXl6uPcZJOxwOdnTEdrt98cxjMESIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCHi7LULc87dGGM3xhibzWZcX1//8qGqvn//Pg6Hw9pjnDQ7+v/msixvvzzn2y//gT59+jQuLy/XHuOkHQ4HOzpiu92Ou7u7+dyZx2CIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFCxNnaA/xOvnz5Mj5+/Lj2GCdtv9+Pz58/rz3Gyfr27duLZ6/GOufcjTF2Y4yx2WzG9fX1z5vsN3NxcTFubm7WHuOkPT09Hf1C8rK5LMvbL8/59st/oJubm3F1dbX2GCdtv9+P8/Pztcc4Wbvdbtzf38/nzryzQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLmsizHL8y5G2Ps/vv4foxx/6uHCvt7jPGw9hAnzo6Oe78sy/lzB6/GytvNOe+WZdmuPccps6Pjju3HYzBEiBUixPpz/bP2AAF2dNyL+/HOChF+WSFCrBAhVogQK0SIFSL+BQyMbf0p5jO5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "envstate, reward, status = qmaze.act(DOWN)  # move down\n",
    "print(reward, qmaze.total_reward)\n",
    "envstate, reward, status = qmaze.act(RIGHT)  # move right\n",
    "print(reward, qmaze.total_reward)\n",
    "envstate, reward, status = qmaze.act(RIGHT)  # move right\n",
    "print(reward, qmaze.total_reward, status)\n",
    "# envstate, reward, status = qmaze.act(RIGHT)  # move right\n",
    "# print(reward, qmaze.total_reward)\n",
    "# envstate, reward, status = qmaze.act(UP)  # move up\n",
    "# print(reward, qmaze.total_reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - 左\n",
    "1 - 向上\n",
    "2 - 右\n",
    "3 - 向下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(s,a)= Q(s, a) + α⋅[Value(s’)+γ⋅maxQ(s′)−Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudocode**\n",
    "```\n",
    "Init Q(s, a) to 0 for all (s, a) pairs\n",
    "Repeat for episode = 1 ... numEpisodes\n",
    "     Initialize s to startState\n",
    "     While (s is not TerminalState)\n",
    "           Choose an action a from Actions(s)\n",
    "           s' = new state after action a from s\n",
    "           Q(s,a)= Q(s, a) + α⋅[Value(s’)+γ⋅maxQ(s′)−Q(s,a)]\n",
    "           s = s'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============episode: 0 ============\n",
      "[[-0.075      -0.35141925  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -5.25\n",
      "\n",
      "============episode: 1 ============\n",
      "[[-0.075      -0.35141925 -0.0076     -0.004     ]\n",
      " [-0.004396   -0.075      -0.004      -0.0076    ]\n",
      " [-0.004       0.          0.          0.        ]\n",
      " [ 0.         -0.004      -0.004       0.        ]\n",
      " [-0.004      -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.1       ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -0.27\n",
      "\n",
      "============episode: 2 ============\n",
      "[[-0.075      -0.39202972 -0.011236   -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.004      -0.0076    ]\n",
      " [-0.004       0.          0.          0.        ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.1       ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -3.2700000000000005\n",
      "\n",
      "============episode: 3 ============\n",
      "[[-0.075      -0.39202972 -0.0145084  -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.0076     -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.0059    ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.19      ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -0.6200000000000001\n",
      "\n",
      "============episode: 4 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 5 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.0148648 ]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.01084    -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.0059    ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.19        0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 6 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.01813072]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0131719  -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.02012   ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.271       0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 7 ============\n",
      "[[-0.075      -0.39202972 -0.02078136 -0.01813072]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.00884812]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0131719  -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.040937  ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.3439      0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 8 ============\n",
      "[[-0.075      -0.39202972 -0.0235654  -0.02407929]\n",
      " [-0.01392385 -0.075      -0.0102559  -0.00884812]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.01357452 -0.0131719  -0.01490008]\n",
      " [-0.007996   -0.004      -0.004       0.040937  ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.00878329  0.0232061  -0.142896  ]\n",
      " [-0.004396    0.          0.40951     0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.64\n",
      "\n",
      "============episode: 9 ============\n",
      "[[-0.075      -0.39202972 -0.02608482 -0.02407929]\n",
      " [-0.01392385 -0.075      -0.0102559  -0.00791055]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.01357452 -0.0131719  -0.01490008]\n",
      " [-0.007996   -0.004      -0.004       0.07338479]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.00878329  0.0232061  -0.142896  ]\n",
      " [-0.004396    0.          0.468559    0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_Episodes = 10\n",
    "MAZE_SIZE = qmaze.maze.shape[0]\n",
    "\n",
    "# 将状态表示为一个数字\n",
    "def state_to_index(state):\n",
    "    return state[0] * MAZE_SIZE + state[1]\n",
    "\n",
    "# 获取当前状态下的最佳行动\n",
    "def get_best_action(rat_row, rat_col):\n",
    "    state = (rat_row, rat_col)\n",
    "    index = state_to_index(state)\n",
    "    return np.argmax(q_table[index])\n",
    "\n",
    "# 通过ε-greedy策略选择下一个行动\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return get_best_action(*state)\n",
    "    \n",
    "# 更新 Q 表格\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    index = state_to_index(state)\n",
    "    next_index = state_to_index(next_state)\n",
    "    q_table[index][action] += LEARNING_RATE * (\n",
    "        reward + DISCOUNT_FACTOR * np.max(q_table[next_index]) - q_table[index][action])\n",
    "\n",
    "    \n",
    "# Q-Table\n",
    "# 每行代表格子，从上往下从左往右\n",
    "# 每列代表动作，0 - 左 1 - 向上 2 - 右 3 - 向下\n",
    "q_table = np.zeros((qmaze.maze.shape[0]**2, 4))  \n",
    "total_reward = []\n",
    "\n",
    "for episode in range(NUM_Episodes):\n",
    "    epsilon = 1.0 / (episode + 1)\n",
    "    \n",
    "    qmaze = Qmaze(maze)\n",
    "    \n",
    "    while qmaze.game_status() == 'not_over':\n",
    "        # 初次观测\n",
    "        rat_row, rat_col, mode = qmaze.state\n",
    "        # 选择动作\n",
    "        # print(rat_row, rat_col, mode)\n",
    "        action = choose_action((rat_row, rat_col), epsilon)\n",
    "        # print(action)\n",
    "        # 执行动作并二次观测\n",
    "        canvas, reward, game_over = qmaze.act(action)\n",
    "        rat_row_next, rat_col_next, mode = qmaze.state\n",
    "        # 根据观测更新Q\n",
    "        update_q_table((rat_row, rat_col), action, (rat_row_next, rat_col_next), reward)\n",
    "    \n",
    "    print('============episode:', episode, '============')\n",
    "    print(q_table)\n",
    "    print('total_reward', qmaze.total_reward)\n",
    "    total_reward.append(qmaze.total_reward)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_value_table\n",
      "[[-0.02407929 -0.00791055  0.02012   ]\n",
      " [-0.0131719   0.07338479  0.271     ]\n",
      " [ 0.0232061   0.468559    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.amax(q_table, axis=1).reshape((3, 3))\n",
    "print('q_value_table')\n",
    "print(q_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af4265edc0>"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEFUlEQVR4nO3ZMUokaQCG4b8WMzFbkAHzMdcDzGn6BHOM9gKewNwDqPleYLPJBsFMU/k3mdBuXRipfnefB/5AqoKPal+sxmXOOYDD98faA4CPEStEiBUixAoRYoUIsULE0b+5eVkW/+fZ48uXL+Pnz59rzzho5+fn4/j4eO0ZB+vHjx/j6elpefPinPPDZ4wxnd1nu92uvuHQz93d3WS3i4uLOXf05zUYIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsEHH03g3LsmzGGJsxxjg9PR03NzefPqrq8fFxbLfbtWcctJeXl3F/f7/2jKY554fPxcXFZLftdjvHGM6ec3d3t/bHdNB+NfZmf16DIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFHaw/g/+Xl5WU8PDysPeNgPT8/77z2bqzLsmzGGJsxxjg9PR339/e/bdh/zdnZ2dhut2vPOGivr697fyHZ7d1Y55zXY4zrMca4vLyc3759++xNWVdXV+P79+9rzzhot7e34+TkZO0ZSb6zQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSKWOef+G5ZlM8bY/Prx6xjj788eFfbnGONp7REHzjPa7+uc8+StC+/Gyscty/LXnPNy7R2HzDPab9/z8RoMEWKFCLH+XtdrDwjwjPbb+Xx8Z4UIf1khQqwQIVaIECtEiBUi/gHp4PY1RUiarwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudocode**\n",
    "```\n",
    "Initialize state-action value function Q(s, a)\n",
    "Initialize sampling counter N(s, a)\n",
    "for episode in episodes:\n",
    "    Generate an episode and record information for each state-action pair\n",
    "    G = 0\n",
    "    for t in reversed(range(0, episode_length)):\n",
    "        S_t, A_t, R_t = episode[t]\n",
    "        G = gamma * G + R_t\n",
    "        N(S_t, A_t) += 1\n",
    "        alpha = 1 / N(S_t, A_t)\n",
    "        Q(S_t, A_t) += alpha * (G - Q(S_t, A_t))\n",
    "return state-action value function Q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1249.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "# 定义参数\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_Episodes = 10\n",
    "MAZE_SIZE = qmaze.maze.shape[0]\n",
    "records = []\n",
    "q_table = np.zeros((qmaze.maze.shape[0]**2, 4))  \n",
    "\n",
    "# 将状态表示为一个数字\n",
    "def state_to_index(state):\n",
    "    return state[0] * MAZE_SIZE + state[1]\n",
    "\n",
    "# 获取当前状态下的最佳行动\n",
    "def get_best_action(rat_row, rat_col):\n",
    "    state = (rat_row, rat_col)\n",
    "    index = state_to_index(state)\n",
    "    return np.argmax(q_table[index])\n",
    "\n",
    "# 通过ε-greedy策略选择下一个行动\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return get_best_action(*state)\n",
    "    \n",
    "# 更新 Q 表格\n",
    "# def update_q_table(state, action, next_state, reward):\n",
    "#     index = state_to_index(state)\n",
    "#     next_index = state_to_index(next_state)\n",
    "#     q_table[index][action] += LEARNING_RATE * (\n",
    "#         reward + DISCOUNT_FACTOR * np.max(q_table[next_index]) - q_table[index][action])\n",
    "\n",
    "    \n",
    "# Q-Table\n",
    "# 每行代表格子，从上往下从左往右\n",
    "# 每列代表动作，0 - 左 1 - 向上 2 - 右 3 - 向下\n",
    "Q = q_table  # 初始化状态-动作价值函数\n",
    "N = np.zeros((qmaze.maze.shape[0]**2, 4))  # 记录每个状态-动作被采样的次数\n",
    "total_reward = []\n",
    "\n",
    "for episode in trange(NUM_Episodes):\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    qmaze = Qmaze(maze)\n",
    "    \n",
    "    # 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "    while qmaze.game_status() == 'not_over':\n",
    "        # 初次观测\n",
    "        rat_row, rat_col, mode = qmaze.state\n",
    "        # 选择动作\n",
    "        # print(rat_row, rat_col, mode)\n",
    "        action = choose_action((rat_row, rat_col), epsilon)\n",
    "\n",
    "        state, action = state_to_index((rat_row, rat_col)), action\n",
    "\n",
    "        # 执行动作并二次观测\n",
    "        canvas, reward, game_over = qmaze.act(action)\n",
    "\n",
    "        records.append((state, action, reward))\n",
    "\n",
    "        rat_row_next, rat_col_next, mode = qmaze.state\n",
    "        \n",
    "\n",
    "    \n",
    "    # 使用经验采样更新状态-动作价值函数\n",
    "    G = 0  # 记录返回值\n",
    "    for j in range(len(records)-1, -1, -1):\n",
    "        state, action, reward = records[j]\n",
    "        G += reward\n",
    "        N[state][action] += 1\n",
    "        alpha = 1 / N[state][action]\n",
    "        Q[state][action] += alpha * (G - Q[state][action])\n",
    "        \n",
    "    # # 根据观测更新Q\n",
    "    # update_q_table((rat_row, rat_col), action, (rat_row_next, rat_col_next), reward)\n",
    "    \n",
    "    # print('============episode:', episode, '============')\n",
    "    # print(Q)\n",
    "    # print('total_reward', qmaze.total_reward)\n",
    "    total_reward.append(qmaze.total_reward)\n",
    "    # print('')\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_value_table\n",
      "[[-12.20965116 -10.07315789   0.        ]\n",
      " [ -6.316       -2.57273973   0.        ]\n",
      " [  0.           1.29333333   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.amax(Q, axis=1).reshape((3, 3))\n",
    "print('q_value_table')\n",
    "print(q_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('q_value_table')\n",
    "# Q = np.array([[0.49107692, 2.30230769, 1.733     ], [2.0725     ,4.33777778  , 2.938       ], [ 0., 4.29777778  , 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20.9964486 , -19.46916667, -17.24666667, -12.20965116],\n",
       "       [-17.30928571, -15.28842105, -15.55846154, -10.07315789],\n",
       "       [-18.505     ,  -8.865     ,  -0.62      ,   0.        ],\n",
       "       [-14.53395833, -12.48782051,  -6.316     , -14.01666667],\n",
       "       [-12.18666667,  -9.71666667,  -6.94767442,  -2.57273973],\n",
       "       [ -7.32      ,  -1.41      ,   0.        ,   0.        ],\n",
       "       [ -5.34      , -13.97666667,   0.        ,   0.        ],\n",
       "       [ -6.88      ,  -2.44318182,   1.29333333,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17bdd529af0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEHUlEQVR4nO3ZMUokaQCG4b8Ws0FMFkzMnbz7AJ7GE5h6g+5Y8BpzAHtyL7CZ6YCZTjjUJhtOtwoj1e/yPFBBUxV8/PhiNT3N8zyA4/fX0gOA9xErRIgVIsQKEWKFCLFCxMlHHp6mye88B3z9+nV8+fJl6RlH7efPn87ogKenp/H8/Dz97t6HYuWwu7u7cXV1tfSMo7bb7ZzRAev1eu89r8EQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiJO3Hpim6XqMcT3GGGdnZ+P29vbTR1X9+PFjbLfbpWcctcvLy7Hb7Zae0TTP87uvMcbs2n9tNpvFNxz79fDwMLPfarWa5z39eQ2GCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRJx85OHVajUeHx8/a0vedrtdesLRe319Hd+/f196xtF6eXnZe+/NWKdpuh5jXI8xxvn5+djtdn9s2P/NxcXF2Gw2S884ar9+/Tr4B8l+b8Y6z/P9GON+jDHW6/V8dXX12ZuyttvtuLm5WXrGUfv27ds4PT1dekaS76wQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiGme58MPTNP1GOP6v4+XY4x/PntU2N9jjOelRxw5Z3TY5TzPp7+78WasvN80TY/zPK+X3nHMnNFhh87HazBEiBUixPpn3S89IMAZHbb3fHxnhQj/WSFCrBAhVogQK0SIFSL+BagGRIe7+P5ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "records = []  # state, action, reward\n",
    "\n",
    "# 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "while qmaze.game_status() == 'not_over':\n",
    "    # 初次观测\n",
    "    rat_row, rat_col, mode = qmaze.state\n",
    "    # 选择动作\n",
    "    # print(rat_row, rat_col, mode)\n",
    "    action = choose_action((rat_row, rat_col), epsilon)\n",
    "    \n",
    "    state, action = state_to_index((rat_row, rat_col)), action\n",
    "    \n",
    "    # 执行动作并二次观测\n",
    "    canvas, reward, game_over = qmaze.act(action)\n",
    "    \n",
    "    records.append((state, action, reward))\n",
    "    \n",
    "    rat_row_next, rat_col_next, mode = qmaze.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, -0.75), (0, 3, -0.04), (3, 2, -0.04), (4, 3, -0.04), (7, 2, 1.0)]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7812117768267714106"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qmaze' object has no attribute 'canvas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [488]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mqmaze\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Qmaze' object has no attribute 'canvas'"
     ]
    }
   ],
   "source": [
    "qmaze.canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def run_maze():\n",
    "    env = MazeEnv(maze_size=3)\n",
    "    num_episodes = 10000\n",
    "    Q = np.zeros((env.maze_size, env.maze_size, 4))  # 初始化状态-动作价值函数\n",
    "    N = np.zeros((env.maze_size, env.maze_size, 4))  # 记录每个状态-动作被采样的次数\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state, Q)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # 使用经验采样更新状态-动作价值函数\n",
    "        G = 0  # 记录返回值\n",
    "        for j in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[j]\n",
    "            G += reward\n",
    "            N[state][action] += 1\n",
    "            alpha = 1 / N[state][action]\n",
    "            Q[state][action] += alpha * (G - Q[state][action])\n",
    "\n",
    "    # 输出最终的状态-动作价值函数\n",
    "    print(\"Final Q:\")\n",
    "    print(Q)\n",
    "\n",
    "def choose_action(state, Q, eps=0.1):\n",
    "    if random.random() < eps:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "        \n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
