{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size, start, end, obstacles):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.obstacles = obstacles\n",
    "        self.grid = np.zeros(size)\n",
    "        self.grid[end] = 1\n",
    "        for obs in obstacles:\n",
    "            self.grid[obs] = -1\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state == self.end\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        next_state = list(state)\n",
    "        if action == 0: # up\n",
    "            next_state[0] = max(0, state[0]-1)\n",
    "        elif action == 1: # down\n",
    "            next_state[0] = min(self.size[0]-1, state[0]+1)\n",
    "        elif action == 2: # left\n",
    "            next_state[1] = max(0, state[1]-1)\n",
    "        elif action == 3: # right\n",
    "            next_state[1] = min(self.size[1]-1, state[1]+1)\n",
    "        \n",
    "        if tuple(next_state) in self.obstacles:\n",
    "            next_state = state\n",
    "\n",
    "        return tuple(next_state)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        if next_state == self.end:\n",
    "            return 1\n",
    "        else:\n",
    "            return -0.01\n",
    "\n",
    "size = (16, 16)\n",
    "start = (0, 0)\n",
    "end = (15, 15)\n",
    "obstacles = [(3, 3), (3, 4), (3, 5), (4, 5), (5, 5), (5, 4), (5, 3)]\n",
    "\n",
    "env = GridWorld(size, start, end, obstacles)\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def first_visit_mc(env, episodes, alpha, gamma, epsilon, epsilon_decay):\n",
    "    Q = np.zeros((size[0], size[1], 4))\n",
    "    returns = {}\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.start\n",
    "        episode_states_actions = []\n",
    "        episode_rewards = []\n",
    "\n",
    "        while not env.is_terminal(state):\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state = env.get_next_state(state, action)\n",
    "            reward = env.get_reward(state, action, next_state)\n",
    "\n",
    "            episode_states_actions.append((state, action))\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode_states_actions))):\n",
    "            state, action = episode_states_actions[t]\n",
    "            G = gamma * G + episode_rewards[t]\n",
    "\n",
    "            if (state, action) not in episode_states_actions[:t]:\n",
    "                if (state, action) not in returns:\n",
    "                    returns[(state, action)] = []\n",
    "                returns[(state, action)].append(G)\n",
    "                Q[state][action] = Q[state][action] + alpha * (G - Q[state][action])\n",
    "\n",
    "        epsilon *= epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-365d2d640cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mepsilon_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfirst_visit_mc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Visualize the final policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dc8aa8069fd6>\u001b[0m in \u001b[0;36mfirst_visit_mc\u001b[0;34m(env, episodes, alpha, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mepisode_states_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_gridworld(env, Q):\n",
    "    grid = np.copy(env.grid)\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            if grid[i, j] == 0:\n",
    "                grid[i, j] = np.argmax(Q[(i, j)])\n",
    "    plt.imshow(grid, cmap=\"cool\")\n",
    "    plt.colorbar(ticks=[-1, 0, 1, 2, 3])\n",
    "    plt.clim(-1, 3)\n",
    "    plt.xticks(range(size[1]), range(size[1]))\n",
    "    plt.yticks(range(size[0]), range(size[0]))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# Train the agent\n",
    "episodes = 5000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.3\n",
    "epsilon_decay = 0.99\n",
    "\n",
    "first_visit_mc(env, episodes, alpha, gamma, epsilon, epsilon_decay)\n",
    "\n",
    "# Visualize the final policy\n",
    "plot_gridworld(env, Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
