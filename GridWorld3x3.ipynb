{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce576c-7389-4cb6-b4d7-e0cc2285c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAP6629 001 RL Spring 2023\n",
    "# Yiran Pang\n",
    "# Copied and adapted from https://www.samyzaf.com/ML/rl/qmaze.html about Maze Definition Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53783cf0-2b09-4181-97b2-eb5fecd7f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591e430-bbd2-44d0-8ced-58eba82d7af5",
   "metadata": {},
   "source": [
    "- 0 - 左\n",
    "- 1 - 向上\n",
    "- 2 - 右\n",
    "- 3 - 向下\n",
    "- 每次移动都会花费老鼠 -0.04 分\n",
    "- 奶酪，给予 1.0 分\n",
    "- 封锁的单元格-0.75 分，动作不会被执行\n",
    "- 已经访问过的单元格，-0.25 分\n",
    "- 总奖励低于负阈值：(-0.5 * maze.size)，lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f2a797-9582-4c5c-9817-7471d8fae6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920ba5a-647c-42b0-955a-4bfd7ec0c97d",
   "metadata": {},
   "source": [
    "## Q-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "1f300d08-5be2-4d39-bdbf-c2ea03e3713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        # 初始化迷宫，老鼠可以从任意位置开始，默认为左上角\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        # 终点始终在右下角\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        # 初始化空格list，maze为1表示空格，为0表示墙体\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        # 将目标格移出空格list\n",
    "        self.free_cells.remove(self.target)\n",
    "        # 检查左上和右下是否为空\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        # 放置老鼠并初始化参数\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        # 初始状态\n",
    "        self.state = (row, col, 'start')\n",
    "        # 设置最低奖励阈值\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        # 初始化总奖励\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        '''\n",
    "            input: action [0, 1, 2, 3] [L, U, R, D]\n",
    "        '''\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "        \n",
    "        # 如果老鼠访问的是空格，则记录\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        # 获取所有可能执行的动作\n",
    "        valid_actions = self.valid_actions()\n",
    "        # print('valid_actions', valid_actions)\n",
    "        \n",
    "        # 如果没有可以执行的动作（被围住了），则状态为 blocked，位置不变\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        # 如果需要执行的动作在可执行动作列表中，那么状态为有效，并相应执行动作\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        # 如果需要执行的动作不在可执行动作列表中（撞墙），位置不变\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            nmode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0  # 奶酪，给予 1.0 分\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        # if (rat_row, rat_col) in self.visited:\n",
    "        #     return -0.25  # 访问已经访问过的单元格，-0.25 分\n",
    "        if mode == 'invalid':\n",
    "            return -0.75  # 撞墙-0.75 分，动作不会被执行\n",
    "        if mode == 'valid':\n",
    "            return -0.04  # 每次移动都会花费老鼠 -0.04 分\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        # 默认验证当前位置\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # 如果在第0行，则不能向上走；如果在最后一行，则不能向下走\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "        # 列-左右\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 如果不在最左列，而左边是墙，则不能向左；右边同理\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        # 上下同理\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        # 返回所有可能执行的动作\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "41424cfe-e758-4c4f-a6d3-b8eb1d19d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "6dbd1d92-88fa-4f83-84f4-424f100b3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  1., 1.],\n",
    "    [ 1.,  1., 1.],\n",
    "    [ 1.,  1., 1.],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "c0199518-6048-470d-98c8-acea5aca9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af423691f0>"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEHUlEQVR4nO3XsW0bWRRA0T8LJYIBGhAWcOhAwRYgFsAanKgDFqVOFGkKYAMLyIAKUEgFjMbJhhZlL6wlr/ecjPg/eHjgxcxMy7IM4Pz9ceoBgB8jVogQK0SIFSLEChFihYiLn7l8eXm5rFar95ol7+rqanz48OHUY5y1l5cXOzri6elpPD8/T987+6lYV6vVuL29/TVT/Ya+fPkyNpvNqcc4a/M829ER6/X61TOvwRAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoWIi7cuTNO0HWNsxxjj48eP4/Pnz+8+VNV+vx/zPJ96jLNmR//etCzLj1+eph+//D/08PAwNpvNqcc4a/M829ER6/V67Ha76XtnXoMhQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKERc/c/nm5mbsdrv3miVvnudTj3D2DofDeHx8PPUYZ+twOLx69mas0zRtxxjbMcb49OmTP+QR+/3eft5wOBzG169fTz1G0puxLstyN8a4G2OM9Xq9bDab954pa57nYT/H3d/fj+vr61OPkeSbFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBBx8daFaZq2Y4ztPz/30zT9/b4jpf05xng+9RBnzo6O++u1g2lZlv9ykN/aNE27ZVnWp57jnNnRccf24zUYIsQKEWL9te5OPUCAHR336n58s0KEJytEiBUixAoRYoUIsULENwMabjSrlY/fAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "714778b4-da62-4a7b-8245-4876b43ab610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af387fb940>"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEOElEQVR4nO3XMUpkWQCG0fuGTkTGoB1wAQa9ASuv3KhN3IGhUW/DDbgac8cFDCIIRoKhHXT0OpmwLbtBefXJOZl1H8XPhQ9fTfM8D2D7/bX0AOD3iBUixAoRYoUIsUKEWCHi0588vLOzM+/t7b3XlrzPnz+P3d3dpWdste/fv7ujDe7v78fT09P0q7M/inVvb2+cnp6+zaoP6OTkZKzX66VnbLWrqyt3tMHR0dGLZ16DIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChGfXntgmqazMcbZGGPs7++P1Wr17qOqnp+fx9XV1dIzttrj4+O4uLhYesbWurm5efFsmuf5t7/o4OBgPj09fYtNH9LJyclYr9dLz9hqFxcX49u3b0vP2GrzPE+/+txrMESIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCHi09IDPpKHh4dxfn6+9Iytdnx8PG5vb5eesbW+fv364tmrsU7TdDbGOBtjjP39/bFard5u2Qezu7vrfl7x48ePcXd3t/SMpFdjnef5coxxOcYYBwcH8/X19buPqlqtVsP9bHZ8fDwODw+XnpHkNytEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghYprnefMD03Q2xjj7/88vY4z/3ntU2D9jjKelR2w5d7TZl3me//7Vwaux8vumafp3nuejpXdsM3e02ab78RoMEWKFCLG+rculBwS4o81evB+/WSHCf1aIECtEiBUixAoRYoWIn0KPagskiYTWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dd8ca-8332-4d43-b44b-864bd4e91ff4",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3300779-b280-40ac-bf99-12e42e854d24",
   "metadata": {},
   "source": [
    "0 - 左\n",
    "1 - 向上\n",
    "2 - 右\n",
    "3 - 向下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9cf1b6-e394-4695-9eb5-fe818d1f5cd8",
   "metadata": {},
   "source": [
    "$$Q(s,a)= Q(s, a) + α⋅[Value(s’)+γ⋅maxQ(s′)−Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e706fa8-35ad-485e-bab8-153676667243",
   "metadata": {},
   "source": [
    "**Pseudocode**\n",
    "```\n",
    "Init Q(s, a) to 0 for all (s, a) pairs\n",
    "Repeat for episode = 1 ... numEpisodes\n",
    "     Initialize s to startState\n",
    "     While (s is not TerminalState)\n",
    "           Choose an action a from Actions(s)\n",
    "           s' = new state after action a from s\n",
    "           Q(s,a)= Q(s, a) + α⋅[Value(s’)+γ⋅maxQ(s′)−Q(s,a)]\n",
    "           s = s'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "3beacf76-c84a-48c4-b9e2-890e81271c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============episode: 0 ============\n",
      "[[-0.075      -0.35141925  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -5.25\n",
      "\n",
      "============episode: 1 ============\n",
      "[[-0.075      -0.35141925 -0.0076     -0.004     ]\n",
      " [-0.004396   -0.075      -0.004      -0.0076    ]\n",
      " [-0.004       0.          0.          0.        ]\n",
      " [ 0.         -0.004      -0.004       0.        ]\n",
      " [-0.004      -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.1       ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -0.27\n",
      "\n",
      "============episode: 2 ============\n",
      "[[-0.075      -0.39202972 -0.011236   -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.004      -0.0076    ]\n",
      " [-0.004       0.          0.          0.        ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.1       ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -3.2700000000000005\n",
      "\n",
      "============episode: 3 ============\n",
      "[[-0.075      -0.39202972 -0.0145084  -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.0076     -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.0059    ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.19      ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward -0.6200000000000001\n",
      "\n",
      "============episode: 4 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.011236  ]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0076     -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.        ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.1         0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 5 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.0148648 ]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.01084    -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.0059    ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.19        0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 6 ============\n",
      "[[-0.075      -0.39202972 -0.01780996 -0.01813072]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.0076    ]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0131719  -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.02012   ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.271       0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 7 ============\n",
      "[[-0.075      -0.39202972 -0.02078136 -0.01813072]\n",
      " [-0.0087088  -0.075      -0.0102559  -0.00884812]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.0083524  -0.0131719  -0.0076    ]\n",
      " [-0.007996   -0.004      -0.004       0.040937  ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.004396   -0.0076     -0.142896  ]\n",
      " [-0.004396    0.          0.3439      0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 8 ============\n",
      "[[-0.075      -0.39202972 -0.0235654  -0.02407929]\n",
      " [-0.01392385 -0.075      -0.0102559  -0.00884812]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.01357452 -0.0131719  -0.01490008]\n",
      " [-0.007996   -0.004      -0.004       0.040937  ]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.00878329  0.0232061  -0.142896  ]\n",
      " [-0.004396    0.          0.40951     0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.64\n",
      "\n",
      "============episode: 9 ============\n",
      "[[-0.075      -0.39202972 -0.02608482 -0.02407929]\n",
      " [-0.01392385 -0.075      -0.0102559  -0.00791055]\n",
      " [-0.004      -0.075      -0.075       0.02012   ]\n",
      " [-0.075      -0.01357452 -0.0131719  -0.01490008]\n",
      " [-0.007996   -0.004      -0.004       0.07338479]\n",
      " [ 0.          0.          0.          0.271     ]\n",
      " [-0.075      -0.00878329  0.0232061  -0.142896  ]\n",
      " [-0.004396    0.          0.468559    0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_Episodes = 10\n",
    "MAZE_SIZE = qmaze.maze.shape[0]\n",
    "\n",
    "# 将状态表示为一个数字\n",
    "def state_to_index(state):\n",
    "    return state[0] * MAZE_SIZE + state[1]\n",
    "\n",
    "# 获取当前状态下的最佳行动\n",
    "def get_best_action(rat_row, rat_col):\n",
    "    state = (rat_row, rat_col)\n",
    "    index = state_to_index(state)\n",
    "    return np.argmax(q_table[index])\n",
    "\n",
    "# 通过ε-greedy策略选择下一个行动\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return get_best_action(*state)\n",
    "    \n",
    "# 更新 Q 表格\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    index = state_to_index(state)\n",
    "    next_index = state_to_index(next_state)\n",
    "    q_table[index][action] += LEARNING_RATE * (\n",
    "        reward + DISCOUNT_FACTOR * np.max(q_table[next_index]) - q_table[index][action])\n",
    "\n",
    "    \n",
    "# Q-Table\n",
    "# 每行代表格子，从上往下从左往右\n",
    "# 每列代表动作，0 - 左 1 - 向上 2 - 右 3 - 向下\n",
    "q_table = np.zeros((qmaze.maze.shape[0]**2, 4))  \n",
    "total_reward = []\n",
    "\n",
    "for episode in range(NUM_Episodes):\n",
    "    epsilon = 1.0 / (episode + 1)\n",
    "    \n",
    "    qmaze = Qmaze(maze)\n",
    "    \n",
    "    while qmaze.game_status() == 'not_over':\n",
    "        # 初次观测\n",
    "        rat_row, rat_col, mode = qmaze.state\n",
    "        # 选择动作\n",
    "        # print(rat_row, rat_col, mode)\n",
    "        action = choose_action((rat_row, rat_col), epsilon)\n",
    "        # print(action)\n",
    "        # 执行动作并二次观测\n",
    "        canvas, reward, game_over = qmaze.act(action)\n",
    "        rat_row_next, rat_col_next, mode = qmaze.state\n",
    "        # 根据观测更新Q\n",
    "        update_q_table((rat_row, rat_col), action, (rat_row_next, rat_col_next), reward)\n",
    "    \n",
    "    print('============episode:', episode, '============')\n",
    "    print(q_table)\n",
    "    print('total_reward', qmaze.total_reward)\n",
    "    total_reward.append(qmaze.total_reward)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "6a82bd75-c918-4a60-ae83-5e7afa05a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_value_table\n",
      "[[-0.02407929 -0.00791055  0.02012   ]\n",
      " [-0.0131719   0.07338479  0.271     ]\n",
      " [ 0.0232061   0.468559    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.amax(q_table, axis=1).reshape((3, 3))\n",
    "print('q_value_table')\n",
    "print(q_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "5b0fdca3-41ee-4182-96a9-c4406904941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af4265edc0>"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEFUlEQVR4nO3ZMUokaQCG4b8WMzFbkAHzMdcDzGn6BHOM9gKewNwDqPleYLPJBsFMU/k3mdBuXRipfnefB/5AqoKPal+sxmXOOYDD98faA4CPEStEiBUixAoRYoUIsULE0b+5eVkW/+fZ48uXL+Pnz59rzzho5+fn4/j4eO0ZB+vHjx/j6elpefPinPPDZ4wxnd1nu92uvuHQz93d3WS3i4uLOXf05zUYIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsEHH03g3LsmzGGJsxxjg9PR03NzefPqrq8fFxbLfbtWcctJeXl3F/f7/2jKY554fPxcXFZLftdjvHGM6ec3d3t/bHdNB+NfZmf16DIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFHaw/g/+Xl5WU8PDysPeNgPT8/77z2bqzLsmzGGJsxxjg9PR339/e/bdh/zdnZ2dhut2vPOGivr697fyHZ7d1Y55zXY4zrMca4vLyc3759++xNWVdXV+P79+9rzzhot7e34+TkZO0ZSb6zQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSKWOef+G5ZlM8bY/Prx6xjj788eFfbnGONp7REHzjPa7+uc8+StC+/Gyscty/LXnPNy7R2HzDPab9/z8RoMEWKFCLH+XtdrDwjwjPbb+Xx8Z4UIf1khQqwQIVaIECtEiBUi/gHp4PY1RUiarwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "4e4c9c4f-5a0b-4e52-a3a8-9e33560fcdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAffklEQVR4nO3deXBU55ku8OfVvqB9Q60NsFnMIrVB4C2LjR3bMbY08RIgS93J3FuezI0zccqT3EmcZFJ39SQz8UzdTCXFeJx7b40DJl4GTEhwsJNx7DgGCauF2DGLWruQ1JJAu/q9f0gCARLqVp/uc06f51elqkiI7tdt95PT3/nOeURVQURE9hVj9gBERBQaBjkRkc0xyImIbI5BTkRkcwxyIiKbizPjSXNzc3XRokVmPDURkW3V1tZeUNW8a39uSpAvWrQINTU1Zjw1EZFticj5mX7OpRUiIpszJMhF5EUR6RCRBiMej4iIAmfUEfn/AfCgQY9FRERBMCTIVfUdAN1GPBYREQUnYmvkIvKkiNSISE1nZ2eknpaIKOpFLMhVdZuqVqpqZV7edbtniIhonrhrhYjI5kzZR05ktP6hUfz8g0ZcGh4zexS6hiszGevKsnBT3gLExIjZ45jCNzCCQ409qDnXgy/eUYbCjGRDH9+QIBeR7QDuBpArIk0A/kZV/8WIxyaai6riW68dxp76Vogzc8KyptcdZCTHY11Z1uWviuJMJCfEmjdcmKgqzl64hJrzPag914Paxh6c7rgIAIiNEVQuyrJmkKvqViMeh2g+dtW1YE99K77xwHJ85Z6bzR6HppkeaofO96DmfA/ePt4BAIiLEaxypWNdWTbWlWWhclEWCtKTTJ44eEOj4zjc3Iuacz2oPd+DQ4096L40AgBIT4rDurIsfObWIqwtzUJFSQZSEoxfCBEzGoIqKyuVl+iTEZp9g3jwH97B8oI0vPzndyDWoR/d7aTn0gg+9PZcDj5Pkw9Do34AQHHWxDJMZVkW1pZlYcXCdMv9O+3sH0bt+W7UTv4fU0NzL0bHJ3J0cW7q5U8clWFYThKRWlWtvPbnXCMn2/L7Fc/srIPfr3h+s9tyb3iaWVZqAjauKMDGFQUAgJExP4629qH2fA9qz3fj/Y+6sKuuBQCQmhCLW0uzLh+xu0sykZYUH7FZ/X7FyY7+idkml0nOdw0AABJiY1BenIE/u2vx5fDOWZAYsdmmY5CTbb3w7hn88Uw3fvB4OUqyU8weh+YpIS4G7pJMuEsy8R8/thiqiqaewclgnzjq/d9vn4JfgRgBli9MR+W0tfbirGSIQSdHLg2PweP1TaxvTy6T9A9NnEDPXZCAtaVZ+PxtpVhXlo3VRelIjLPGGj+XVsiWjrX2ofrH7+GeFXn46RfWGfZGJmvqHxpFndeHmnMT4fphow8XJ3coFaQnTob6xFr7Klc64mMD21nd4huctn7fjWOt/Rj3K0SAZflpWDu5RLKuLAtlOSmm/3c229IKg5xsZ2h0HNU/fg/dAyPY9/QnkJ2aYPZIFGHjfsWJtn7Unu++fPTc1DMIAEiKj0F5cSYqJ5dj1pZmITMlAWPjfhxv60fNue7L4d3SOwQASI6Phbskc+L3yyb+TkZy5JZwAsUgp6jx3/ccxQvvnsXPvrQe9yzPN3scsoj2vqHLJ1Brz3fjSEsfxvwT+bYoJwUd/cMYGBkHABRmJE07KZmNWwrTEBfgUbyZeLKTosIfTl/AC++exRdvL2OI01UK0pOwqbwQm8oLAQCDI+PwNPlQe74H9U0+fHJZHtYtykZlWRZcmcbu4zYbg5xso3dgFM/8woMlean49kO3mD0OWVxyQixuX5KD25fkmD1K2DHIyTa+u6sBnf3DeO0/3xmVVwQSzZf1F4WIAOyqa8ZuTwu+du9SlBdnmj0OkaUwyMnyWnyD+M6/NWBtaSb+4u6bzB6HyHIY5GRpE1dvejA+efWmHXYWEEUa3xVkaS++dxbvn+nC9x5eibKcVLPHIbIkBjlZ1vG2Pvzg1ydw3y0F2Ly+xOxxiCyLQU6WNDw2jqd31CE9OQ7PPbbG9EujiayM2w/Jkv7+zZM43taPF/+0Erkm3VGOyC54RE6W8/5HXfjn35/B524rvXyrUyKaHYOcLKV3cBTP7KzDopxUfGcTr94kCgSXVshS/mZXA9r7h/HqX9wZlkosomjEI3KyjDc8Lfi3uhZ8dePNcJdkmj0OkW0wyMkSWnsH8ezrh+EuycRTLFAmCgqDnEzn9yv+6hcejI7z6k2i+eA7hkz3sz+cw3unu/Ddh1dicS6v3iQKliFBLiIPisgJETktIn9txGOSM5xo68ff/vo47l2Rj60bePUm0XyEHOQiEgvgnwB8GsBKAFtFZGWoj0vRb3hsHE+/XIe0xDg891g5r94kmicjjsg3ADitqmdUdQTADgDVBjwuRbkf/eYkjrX24W8fK0deGq/eJJovI4K8CIB32vdNkz8jmtUfz3Rh2ztnsHVDCe5byas3iUJhRJDP9HlYr/slkSdFpEZEajo7Ow14WrKrvqFRPLPTg7LsFHxnE1fhiEJlRJA3AZh+lqoYQMu1v6Sq21S1UlUr8/LyDHhasqvv7zqCtr4h/GizG6mJvHqTKFRGBPlBAEtFZLGIJADYAmC3AY9LUeiX9a147cNmfOWem7G2NMvscYiiQsiHQ6o6JiJPAdgHIBbAi6p6JOTJKOq09Q7h268fRkVxBr66kVdvEhnFkM+1qroXwF4jHouik9+v+MYrHoyM+fH8ZjfiefUmkWH4bqKI+L/vn8PvT13As5tuwZK8BWaPQxRVGOQUdqfa+/Hcr45j44p8fP62UrPHIYo6DPJ5ONHWj1Pt/WaPYQsjY358bUcdUhPZvUkULgzyefj6y3X48r/WQvW67fJ0jef3n8TR1j489+ga5KclmT0OUVRikAdpYGQMx9v68FHnJRxp6TN7HEs7cLYbP/33j7C5sgT3r1po9jhEUYtBHqSG5j74Jw/E3/Bcd90TTeofGsXXX65DSVYKvvsIr94kCicGeZA8Xh8AwF2Sid2eFvj9XF6Zyfd3H0Vr7yCe3+zGAl69SRRWDPIg1TX5UJyVjD+9cxFae4dQc77H7JEs51eHW/HqoSZ85Z6bsa6MV28ShRuDPEgerw8VJZn41MoCJMXHYFdds9kjWUp73xC+9fphlBdn4C/vXWr2OESOwCAPwoWLw2jqGYS7OBOpiXG475YC7D3citFxv9mjWYKq4huv1GNodJxXbxJFEN9pQahv8gEAKkoyAQDV7iL0DIzi3VMXzBvKQv7f++fxzslOPPvQLbiJV28SRQyDPAh13l7ECLC6KB0A8IlluUhPisNu7l7B6Y5+/M+9x/DJZXn4wu1lZo9D5CgM8iDUN/mwrCANKQkTuzAS42Lx0JpC7DvShsGRcZOnM8/ImB9Pv1yHlIRY/PBxdm8SRRqDPECqCo/XB/fkssqUqgoXBkbG8dbxdnMGs4B/fOskGpr78L8eXYP8dF69SRRpDPIAebsH0TMwenl9fMptS3KQn5aIXXXOXF45e+ESfvK7j/D4umI8uLrQ7HGIHIlBHqC6yROd5cUZV/08NkbwcLkL/36iE70DoyZMZq4dBxohIvjmA8vNHoXIsRjkAfJ4fUiKj8GygrTr/qza7cLIuB+/PtJqwmTmGRnz45XaJty7Ip9LKkQmYpAHyOP1YbUrY8a90eXFGSjLSXHc7pX9x9rRdWkEWzfwHuNEZmKQB2B03I+Glt7r1seniAiqK1z4w0dd6OgbiuxwJtp+oBGujCR8Ylme2aMQORqDPAAn2/sxNOqfNcgBoMrtgiqwp94Zyyve7gH8/tQFfHZ9CWJjuN2QyEwM8gB4vL0AAHdx5qy/c3N+GlYWpmOXQ5ZXXj7oRYwAn60sMXsUIsdjkAfA4/UhKyUeJdnJN/y9KrcLHq8P57suRWgyc4yN+7Gzxou7l+fDlXnj14SIwo9BHgBP08QdD+e6YvGRChcAYHeU7yl/+3gHOvqHsWU9j8aJrIBBPoeBkTGcbO9HxQ2WVaYUZSZj/aIs7PK0RHWf546DXuSnJWLjinyzRyEihBjkIvKEiBwREb+IVBo1lJVMVbtde2n+bKrcRTjdcRHHWvvDO5hJWnyD+N2JDjxRWYw43qaWyBJCfSc2AHgUwDsGzGJJU9Vu117ROZuHVi9EbIxE7Z7ynTVe+BXYsp57x4msIqQgV9VjqnrCqGGsaKraLWdBYkC/n7MgER9fmos3orDPc9yv2HnQi48vzUVJdorZ4xDRpIh9NhaRJ0WkRkRqOjs7I/W0IZuqdgtGVYULzb5BHGqMrj7Pd052oqV3iEfjRBYzZ5CLyH4RaZjhqzqYJ1LVbapaqaqVeXn2uBJwerVbMO5ftRCJcTFRd0fE7QcakZOagE+tLDB7FCKaJm6uX1DV+yIxiBVdW+0WqAXT+jy/98jKqOiu7OgbwlvHO/CfPrYYCXH2/+chiiZ8R97AtdVuwahyu9B1aQTvnY6OPs9f1DZh3K/YzL3jRJYT6vbDz4hIE4A7APxSRPYZM5Y1eLxXV7sF4+7leUiLkj5Pv1+x42Ajbl+SjSUsVSaynFB3rbyuqsWqmqiqBar6gFGDmU1V4Wm6vtotUIlxsfj06oXY19CGoVF793n+4aMueLsHebtaIovi0sosGrsH4Juh2i0YVRVFuDQyjrePdxg3mAm2H2xEZko8Hli10OxRiGgGDPJZeJom7ngYyKX5s7njphzkLkjErrpmg6aKvK6Lw3jzSBsevbUYSfGxZo9DRDNgkM/iSrXb/NeEJ/o8C/HbE53oHbRnn+erh5owOq7YuoEnOYmsikE+C4/XhzVFGSHfT6Ta7cLImB/7jrQZNFnkqCp2HPBiXVkWls7QVUpE1sAgn8HlarcQllWmuEsyUZqdgjdsuHvlg7PdOHPhEk9yElkcg3wGU9Vu5SGc6JwiIqiqcOG90xfQ0W+vPs8dBxqRlhSHTWsKzR6FiG6AQT6DQKrdglHldsGvwF4b9Xn6Bkawt6ENf+IuQnICT3ISWRmDfAaBVrsFallBGlYsTLNVn+drh5oxMubnsgqRDTDIZxBotVswqtwufNjoQ2PXgGGPGS6qE1dyVhRnYKUr+NsTEFFkMcivcWk48Gq3YDxSPtHn+Ua99Y/KDzX6cLL9IrbwaJzIFhjk12ho7g2q2i1QJdkpWFeWZYti5u0HGpGaEHu5TJqIrI1Bfg3P5K1rA612C0a124UT7f043tZn+GMbpW9oFHvqW1DldmFBYvA3CyOiyGOQX8PT1IuS7MCr3YLx0JrCiT5PCx+V76prwdCony1ARDbCIL+Gx+szfH18Su6CRNx1cy52e1qgar0+T1XF9g8asbIwPSyfSIgoPBjk01yudjN4fXy6qgoXmnoGcajRF7bnmK/Dzb042tqHrRtKDN2xQ0ThxSCfZr7VbsF4YFUBEuJiLHnJ/vYDXiTFx6D61iKzRyGiIDDIp5mqdlsVxr3TaUnxuHdFPvbUt2Bs3B+25wnWpeEx7K5rxsPlLqQnxZs9DhEFgUE+TSjVbsGodrtw4eII3j/TFdbnCcYbnhZcGhnn7WqJbIhBPinUardg3L08H2mJcdhlod0r2w96sTR/AdaWZpk9ChEFiUE+yYhqt0AlxcfiAQv1eR5t6YPH68PWDaU8yUlkQwzySXVeH4DQqt2CUVXhQv/wGH53wvw+zx0HG5EQF4NH1/IkJ5EdMcgneby9IVe7BePOm3KQuyABu03evTI4Mo7XP2zGp1cvRGZKgqmzENH8MMgneZqMqXYLVFxsDDatKcT+Yx3oHzKvz3Pv4Vb0D43xSk4iGwsptUTkhyJyXETqReR1Eck0aK6IGh3344hB1W7BqHIXYWTMjzePtEf0eafbfqARi3NTcfuSbNNmIKLQhHr4+RsAq1W1HMBJAN8KfaTIm6p2i8SJzunWlmaiOCvZtMKJU+39qDnfgy3reSUnkZ2FFOSq+qaqjk1++0cAxaGPFHmXq90iHOTT+zwvXByO6HMDwI6DXsTHCh5bZ8t/bUQ0ycgF4T8D8KvZ/lBEnhSRGhGp6ezsNPBpQ+fx+pCdmoDiLGOq3YJR5XZh3K/YeziyfZ5Do+N49VAT7l+5ELlhuNMjEUXOnEEuIvtFpGGGr+ppv/MsgDEAL832OKq6TVUrVbUyLy/PmOkN4mnyobw4w5TlhRUL07G8IC3it7bdd6QNvoFRbOGVnES2N+e16Kp6343+XET+A4CHAdyrVrw36xymqt0eWLXQtBmq3C78cN8JNPUMoDgrJSLPueOAFyXZybjrptyIPB8RhU+ou1YeBPBfAFSpqvVbhWcQrmq3YFRNVqq94YnM8srZC5fw/pkubFlfipgYnuQksrtQ18h/DCANwG9EpE5EfmrATBEVzmq3QJVkp+DW0kzsqmuOyPPtONiI2BjBEzzJSRQVQt21crOqlqiqe/Lry0YNFikeb/iq3YJRXeHC8bZ+nGzvD+vzjIz58WptEzauyEd+elJYn4uIIsPxV3bWhbHaLRibyl2IEYT9pOdbx9px4eIIPreBV3ISRQtHB/mFi8No9oW32i1QeWmR6fP8+YFGuDKS8Ill1to5RETz5+ggj0S1WzAeqXChsXvg8p0YjebtHsC7py/gicoSxPIkJ1HUcHSQ13l7ERsjYa12C8aDqxciIS4mbHdE3FnjhQD47HruHSeKJo4O8khVuwUqPSke9yzPw576Voz7jV1eGRv3Y2eNF59cloeizMhfwUpE4ePYIL9S7WbetsOZVLuL0Nk/jD8a3Of52xOdaO8bxhae5CSKOo4N8qlqt3IL7FiZbuOKfCxIjDN8T/mOA43IT0vExhX5hj4uEZnPsUEe6Wq3QCXFx+L+VQX4VUMbhseM6fNs7R3Eb0904InKYsRHqDiDiCLHse/qSFe7BaOqwoX+oTH87oQxd4ncebAJfgU2V3JZhSgaOTfII1ztFoy7bs5FTqoxfZ7jfsXOGi8+vjQXpTmRuSEXEUWW9VIsAkbH/Whojny1W6DiY2Pw0JpC7D/ajovDY3P/hRt451Qnmn2D7OQkimKODPITbf0YHot8tVswqt0uDI/58ZujbSE9zo4DjchJTcCnVhYYNBkRWY0jg3zqjodWuDR/NmtLs1CUmYxdIdx7paNvCG8d68Dj64qREOfIf9VEjuDId3e9t9e0ardAxcQIHqlw4fenLqBrnn2ev6htwphfsZlXchJFNUcGuafJhwqTqt2CUVUx2efZEPzyit+vePmgF7ctzsaSPOvtzCEi4zguyKeq3ay8Pj7llsI0LM1fgDfmsbzy/pkuNHYP4HO38SQnUbRzXJBPVbvZIchFBFUVLhw4141m32BQf3f7gUZkpsSb2kVKRJHhuCCfOtFp1a2H16pyT/R57gliT3nXxWHsO9KGz9xahKT42HCNRkQW4bwgn6x2y05NMHuUgJTlpKKiJDOo3SuvHWrG6LhiK2+QReQIjgtyq1S7BaO6woWjrX043TF3n6eqYvvBRqwry8KygrQITEdEZnNUkHf2W6faLRgPlxcG3Od54Gw3znRewhZuOSRyDEcFudWq3QKVn56EO27KCajPc8dBL9IS47CpvDBC0xGR2RwV5B6vz1LVbsGoqnDhXNcADjf3zvo7voER/PJwK/7k1iLLtB4RUfiFFOQi8t9EpF5E6kTkTRFxGTVYONQ19Vqq2i0YD64qREJszA1Per7+YTNGxvzYsoHLKkROEuoR+Q9VtVxV3QD2APhe6COFh6qi3oLVboHKSInHJ5fnYU99y4x9nqqKHQe8KC/OwCqXPf8ZiWh+QgpyVe2b9m0qAGMbgw00Ve1mtx0r01W7XWjvG8YHZ6/v8/zQ68OJ9n5uOSRyoJDXyEXkf4iIF8DncYMjchF5UkRqRKSms9OY5ptgXK52s9mJzunuXVGA1IRYvDHDxUHbP2hESkIsHqmw9OoWEYXBnEEuIvtFpGGGr2oAUNVnVbUEwEsAnprtcVR1m6pWqmplXl6ecf8EAfJ4e5EcH4ul+fa9gVRyQizuX7UQew+3YWTMf/nn/UOj2FPfiqoKFxYk2m/9n4hCM2eQq+p9qrp6hq9d1/zqzwE8Fp4xQ+dp8mF1Ubolq92CUVXhQu/gKN45eeVTza66FgyOjnNZhcihQt21snTat1UAjoc2TnhYvdotGB9bmouslHjsmra8sv1AI24pTEd5MU9yEjlRqJ/DnxOR5QD8AM4D+HLoIxnPDtVugZrq83ztUDMuDY/hTOclHGnpw3+tXmX5+6sTUXiEFOSqatmllOnsUO0WjGp3EV76oBH7j7Xjg7PdSIqPQbW7yOyxiMgkjjgz5vH6LF/tFozKsiwUZiRhxwEvDjf3YtMaFzKS480ei4hMYu8zfwHyeHttUe0WqJiYicKJ98904eLwGLbySk4iR4v6IL84PIaTHfaodgvG1H7xpfkLsK4sy+RpiMhMUb+00tDcC7VJtVswVrnS8dnKYmxckR81nzSIaH6iPsjrbVbtFigRwQ8erzB7DCKygKhfWvF4e1GanWKbajciomBFfZDXeX1Rt6xCRDRdVAf5VLVbBa94JKIoFtVBbtdqNyKiYER1kNu52o2IKFBRHeR2rnYjIgpU1Aa5qsLjtW+1GxFRoKI2yM93DaB30N7VbkREgYjaIPfwRCcROUTUBnmd12f7ajciokBEbZDXN/ViTVGG7avdiIjmEpUpd7najSc6icgBojLIo6najYhoLlEZ5J4oveMhEdFMojPIo6zajYjoRqI0yKOr2o2I6EaiLsijtdqNiGg2URfk0VrtRkQ0G0OCXET+SkRURHKNeLxQeLw+ADzRSUTOEXKQi0gJgE8BaAx9nNB5mnysdiMiRzHiiPx5AN8EoAY8Vsg83l4uqxCRo4QU5CJSBaBZVT0GzRMSVrsRkRPN2bggIvsBLJzhj54F8G0A9wfyRCLyJIAnAaC0tDSIEQM3Ve3m5hE5ETnInEGuqvfN9HMRWQNgMQDP5H7tYgCHRGSDqrbN8DjbAGwDgMrKyrAsw1ypduMRORE5x7w70FT1MID8qe9F5ByASlW9YMBc8zJV7ZacEGvWCEREERc1+8hZ7UZETmVYK7GqLjLqseaD1W5E5FRRc0TOajcicqqoCXJWuxGRU0VNkHu8Pla7EZEjRUXqjY770dDSx2o3InKkqAjyE239GGG1GxE5VFQEOavdiMjJoiPIvT7ksNqNiBwqSoJ84o6HrHYjIieyfZBPVbuV846HRORQtg9yVrsRkdPZPshZ7UZETmf/IGe1GxE5nP2DnNVuRORwtg7yjv4hVrsRkePZOsjrvb0AWO1GRM5m6yD3NLHajYjI5kHei+WsdiMih7NtkE9Vu/FEJxE5nW2DfKrajR2dROR0tg3yqTselvNCICJyONsGOavdiIgm2DbIWe1GRDTBlinIajcioitsGeSsdiMiuiKkIBeR74tIs4jUTX49ZNRgN1LHOx4SEV0WZ8BjPK+qf2fA4wSM1W5ERFfYcmnF0+RjtRsR0SQjgvwpEakXkRdFJGu2XxKRJ0WkRkRqOjs75/1kF4fHcKrjIpdViIgmzRnkIrJfRBpm+KoG8BMANwFwA2gF8PezPY6qblPVSlWtzMvLm/fAV6rduGOFiAgIYI1cVe8L5IFE5J8B7Al5ojmw2o2I6Gqh7lopnPbtZwA0hDbO3Kaq3bJY7UZEBCD0XSs/EBE3AAVwDsCfhzrQXDzeXqwtm3UpnojIcUIKclX9olGDBGKq2u1Ldy2K5NMSEVmarbYfstqNiOh6tgpyVrsREV3PVkFenJWMx9cWs9qNiGgaIy7Rj5jN60uxeX2p2WMQEVmKrY7IiYjoegxyIiKbY5ATEdkcg5yIyOYY5ERENscgJyKyOQY5EZHNMciJiGxOVDXyTyrSCeD8PP96LoALBo5jd3w9ruBrcTW+HleLhtejTFWva+YxJchDISI1qlpp9hxWwdfjCr4WV+PrcbVofj24tEJEZHMMciIim7NjkG8zewCL4etxBV+Lq/H1uFrUvh62WyMnIqKr2fGInIiIpmGQExHZnK2CXEQeFJETInJaRP7a7HnMIiIlIvJbETkmIkdE5Gtmz2QFIhIrIh+KyB6zZzGbiGSKyCsicnzyv5M7zJ7JLCLy9cn3SYOIbBeRJLNnMpptglxEYgH8E4BPA1gJYKuIrDR3KtOMAXhGVW8BcDuArzj4tZjuawCOmT2ERfwjgF+r6goAFXDo6yIiRQD+EkClqq4GEAtgi7lTGc82QQ5gA4DTqnpGVUcA7ABQbfJMplDVVlU9NPm/+zHxJi0ydypziUgxgE0AXjB7FrOJSDqATwD4FwBQ1RFV9Zk6lLniACSLSByAFAAtJs9jODsFeREA77Tvm+Dw8AIAEVkE4FYAH5g8itn+AcA3AfhNnsMKlgDoBPCzyaWmF0Qk1eyhzKCqzQD+DkAjgFYAvar6prlTGc9OQS4z/MzReydFZAGAVwE8rap9Zs9jFhF5GECHqtaaPYtFxAFYC+AnqnorgEsAHHlOSUSyMPHJfTEAF4BUEfmCuVMZz05B3gSgZNr3xYjCj0iBEpF4TIT4S6r6mtnzmOwuAFUicg4TS24bReRfzR3JVE0AmlR16lPaK5gIdie6D8BZVe1U1VEArwG40+SZDGenID8IYKmILBaRBEycsNht8kymEBHBxPrnMVX9kdnzmE1Vv6Wqxaq6CBP/XbytqlF31BUoVW0D4BWR5ZM/uhfAURNHMlMjgNtFJGXyfXMvovDEb5zZAwRKVcdE5CkA+zBx5vlFVT1i8lhmuQvAFwEcFpG6yZ99W1X3mjcSWcxXAbw0edBzBsCXTJ7HFKr6gYi8AuAQJnZ7fYgovFSfl+gTEdmcnZZWiIhoBgxyIiKbY5ATEdkcg5yIyOYY5ERENscgJyKyOQY5EZHN/X/8fgDk3lcpggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41548992-ebc4-4ccb-b468-35e57301f656",
   "metadata": {},
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c522046-2190-4ac9-90fb-1e95987bba2e",
   "metadata": {},
   "source": [
    "**Pseudocode**\n",
    "```\n",
    "Initialize state-action value function Q(s, a)\n",
    "Initialize sampling counter N(s, a)\n",
    "for episode in episodes:\n",
    "    Generate an episode and record information for each state-action pair\n",
    "    G = 0\n",
    "    for t in reversed(range(0, episode_length)):\n",
    "        S_t, A_t, R_t = episode[t]\n",
    "        G = gamma * G + R_t\n",
    "        N(S_t, A_t) += 1\n",
    "        alpha = 1 / N(S_t, A_t)\n",
    "        Q(S_t, A_t) += alpha * (G - Q(S_t, A_t))\n",
    "return state-action value function Q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "2e63c844-7c67-44ce-b385-31238d9e2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============episode: 0 ============\n",
      "[[-7.54  -5.61  -4.07  -9.16 ]\n",
      " [-4.07   0.     0.    -3.99 ]\n",
      " [ 0.    -1.915 -3.455 -1.955]\n",
      " [-8.81  -6.4   -8.68  -7.31 ]\n",
      " [ 0.     0.    -3.95  -8.64 ]\n",
      " [ 0.    -3.495 -0.75   0.   ]\n",
      " [-7.27  -6.44  -6.52   0.   ]\n",
      " [-6.48   0.    -8.6    0.   ]\n",
      " [ 0.     0.     0.     0.   ]]\n",
      "total_reward -4.86\n",
      "\n",
      "============episode: 1 ============\n",
      "[[-7.22       -5.29       -3.75       -6.944     ]\n",
      " [-3.75        0.          0.         -2.19333333]\n",
      " [ 0.         -1.595      -3.135      -1.635     ]\n",
      " [-8.49       -6.08       -3.78       -6.99      ]\n",
      " [ 0.88        0.72       -3.63       -3.72      ]\n",
      " [ 0.         -3.175      -0.43        0.        ]\n",
      " [-6.95       -6.12       -6.2         0.        ]\n",
      " [-6.16        0.84       -5.18666667  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.64\n",
      "\n",
      "============episode: 2 ============\n",
      "[[-6.82       -4.89       -3.35       -5.28888889]\n",
      " [-3.35        0.          0.         -1.482     ]\n",
      " [ 0.         -1.195      -2.735      -1.235     ]\n",
      " [-8.09       -5.68       -2.25       -6.59      ]\n",
      " [ 1.32        1.16       -3.23       -2.19      ]\n",
      " [ 0.         -2.775      -0.03        0.        ]\n",
      " [-6.55       -5.72       -5.8         0.        ]\n",
      " [-5.76        1.28       -3.29333333  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 3 ============\n",
      "[[-6.42       -4.49       -2.95       -4.03428571]\n",
      " [-2.95        0.          0.         -0.715     ]\n",
      " [ 0.         -0.795      -2.335      -0.835     ]\n",
      " [-7.69       -5.28       -1.29538462 -6.19      ]\n",
      " [ 1.73333333  1.4        -2.83       -1.23076923]\n",
      " [ 0.         -2.375       0.37        0.        ]\n",
      " [-6.15       -5.32       -5.4         0.        ]\n",
      " [-5.36        1.69333333 -2.056       0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.8\n",
      "\n",
      "============episode: 4 ============\n",
      "[[-6.004      -4.074      -2.534      -3.004     ]\n",
      " [-2.534       0.          0.         -0.12636364]\n",
      " [ 0.         -0.379      -1.919      -0.419     ]\n",
      " [-7.274      -4.864      -0.54526316 -5.774     ]\n",
      " [ 2.16        1.77333333 -2.414      -0.48      ]\n",
      " [ 0.         -1.959       0.786       0.        ]\n",
      " [-5.734      -4.904      -4.984       0.        ]\n",
      " [-4.944       2.12       -1.12266667  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 5 ============\n",
      "[[-5.58000000e+00 -3.65000000e+00 -2.11000000e+00 -2.13037037e+00]\n",
      " [-2.11000000e+00  0.00000000e+00  0.00000000e+00  3.98571429e-01]\n",
      " [ 0.00000000e+00  4.50000000e-02 -1.49500000e+00  5.00000000e-03]\n",
      " [-6.85000000e+00 -4.44000000e+00  8.92307692e-02 -5.35000000e+00]\n",
      " [ 2.59200000e+00  2.18000000e+00 -1.99000000e+00  1.53846154e-01]\n",
      " [ 0.00000000e+00 -1.53500000e+00  1.21000000e+00  0.00000000e+00]\n",
      " [-5.31000000e+00 -4.48000000e+00 -4.56000000e+00  0.00000000e+00]\n",
      " [-4.52000000e+00  2.55200000e+00 -3.67619048e-01  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 6 ============\n",
      "[[-5.15142857 -3.22142857 -1.68142857 -1.36914286]\n",
      " [-1.68142857  0.          0.          0.89352941]\n",
      " [ 0.          0.47357143 -1.06642857  0.43357143]\n",
      " [-6.42142857 -4.01142857  0.64941176 -4.92142857]\n",
      " [ 3.02666667  2.6        -1.56142857  0.71294118]\n",
      " [ 0.         -1.10642857  1.63857143  0.        ]\n",
      " [-4.88142857 -4.05142857 -4.13142857  0.        ]\n",
      " [-4.09142857  2.98666667  0.27428571  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 7 ============\n",
      "[[-4.72       -2.79       -1.25       -0.69090909]\n",
      " [-1.25        0.          0.          1.372     ]\n",
      " [ 0.          0.905      -0.635       0.865     ]\n",
      " [-5.99       -3.58        1.15813953 -4.49      ]\n",
      " [ 3.46285714  3.02666667 -1.13        1.22046512]\n",
      " [ 0.         -0.675       2.07        0.        ]\n",
      " [-4.45       -3.62       -3.7         0.        ]\n",
      " [-3.66        3.42285714  0.84        0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 8 ============\n",
      "[[-4.28666667 -2.35666667 -0.81666667 -0.07555556]\n",
      " [-0.81666667  0.          0.          1.84043478]\n",
      " [ 0.          1.33833333 -0.20166667  1.29833333]\n",
      " [-5.55666667 -3.14666667  1.62943396 -4.05666667]\n",
      " [ 3.9         3.45714286 -0.69666667  1.69056604]\n",
      " [ 0.         -0.24166667  2.50333333  0.        ]\n",
      " [-4.01666667 -3.18666667 -3.26666667  0.        ]\n",
      " [-3.22666667  3.86        1.352       0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n",
      "============episode: 9 ============\n",
      "[[-3.852      -1.922      -0.382       0.49107692]\n",
      " [-0.382       0.          0.          2.30230769]\n",
      " [ 0.          1.773       0.233       1.733     ]\n",
      " [-5.122      -2.712       2.0725     -3.622     ]\n",
      " [ 4.33777778  3.89       -0.262       2.1325    ]\n",
      " [ 0.          0.193       2.938       0.        ]\n",
      " [-3.582      -2.752      -2.832       0.        ]\n",
      " [-2.792       4.29777778  1.82472727  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "total_reward 0.88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "NUM_Episodes = 10\n",
    "MAZE_SIZE = qmaze.maze.shape[0]\n",
    "\n",
    "# 将状态表示为一个数字\n",
    "def state_to_index(state):\n",
    "    return state[0] * MAZE_SIZE + state[1]\n",
    "\n",
    "# 获取当前状态下的最佳行动\n",
    "def get_best_action(rat_row, rat_col):\n",
    "    state = (rat_row, rat_col)\n",
    "    index = state_to_index(state)\n",
    "    return np.argmax(q_table[index])\n",
    "\n",
    "# 通过ε-greedy策略选择下一个行动\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return get_best_action(*state)\n",
    "    \n",
    "# 更新 Q 表格\n",
    "def update_q_table(state, action, next_state, reward):\n",
    "    index = state_to_index(state)\n",
    "    next_index = state_to_index(next_state)\n",
    "    q_table[index][action] += LEARNING_RATE * (\n",
    "        reward + DISCOUNT_FACTOR * np.max(q_table[next_index]) - q_table[index][action])\n",
    "\n",
    "    \n",
    "# Q-Table\n",
    "# 每行代表格子，从上往下从左往右\n",
    "# 每列代表动作，0 - 左 1 - 向上 2 - 右 3 - 向下\n",
    "Q = np.zeros((qmaze.maze.shape[0]**2, 4))  # 初始化状态-动作价值函数\n",
    "N = np.zeros((qmaze.maze.shape[0]**2, 4))  # 记录每个状态-动作被采样的次数\n",
    "total_reward = []\n",
    "\n",
    "for episode in range(NUM_Episodes):\n",
    "    epsilon = 1.0 / (episode + 1)\n",
    "    \n",
    "    qmaze = Qmaze(maze)\n",
    "    \n",
    "    # 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "    while qmaze.game_status() == 'not_over':\n",
    "        # 初次观测\n",
    "        rat_row, rat_col, mode = qmaze.state\n",
    "        # 选择动作\n",
    "        # print(rat_row, rat_col, mode)\n",
    "        action = choose_action((rat_row, rat_col), epsilon)\n",
    "\n",
    "        state, action = state_to_index((rat_row, rat_col)), action\n",
    "\n",
    "        # 执行动作并二次观测\n",
    "        canvas, reward, game_over = qmaze.act(action)\n",
    "\n",
    "        records.append((state, action, reward))\n",
    "\n",
    "        # rat_row_next, rat_col_next, mode = qmaze.state\n",
    "    \n",
    "    # 使用经验采样更新状态-动作价值函数\n",
    "    G = 0  # 记录返回值\n",
    "    for j in range(len(records)-1, -1, -1):\n",
    "        state, action, reward = records[j]\n",
    "        G += reward\n",
    "        N[state][action] += 1\n",
    "        alpha = 1 / N[state][action]\n",
    "        Q[state][action] += alpha * (G - Q[state][action])\n",
    "        \n",
    "    # # 根据观测更新Q\n",
    "    # update_q_table((rat_row, rat_col), action, (rat_row_next, rat_col_next), reward)\n",
    "    \n",
    "    print('============episode:', episode, '============')\n",
    "    print(Q)\n",
    "    print('total_reward', qmaze.total_reward)\n",
    "    total_reward.append(qmaze.total_reward)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "4faf13c7-bef1-4f3b-8ede-c06d7627d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_value_table\n",
      "[[-0.02407929 -0.00791055  0.02012   ]\n",
      " [-0.0131719   0.07338479  0.271     ]\n",
      " [ 0.0232061   0.468559    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "q_value_table = np.amax(q_table, axis=1).reshape((3, 3))\n",
    "print('q_value_table')\n",
    "print(q_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "14dcc69f-c5c0-44c5-b550-7f56233f5fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af426c3c10>"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEHUlEQVR4nO3ZMUokaQCG4b8Ws0FMFkzMnbz7AJ7GE5h6g+5Y8BpzAHtyL7CZ6YCZTjjUJhtOtwoj1e/yPFBBUxV8/PhiNT3N8zyA4/fX0gOA9xErRIgVIsQKEWKFCLFCxMlHHp6mye88B3z9+nV8+fJl6RlH7efPn87ogKenp/H8/Dz97t6HYuWwu7u7cXV1tfSMo7bb7ZzRAev1eu89r8EQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiJO3Hpim6XqMcT3GGGdnZ+P29vbTR1X9+PFjbLfbpWcctcvLy7Hb7Zae0TTP87uvMcbs2n9tNpvFNxz79fDwMLPfarWa5z39eQ2GCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRJx85OHVajUeHx8/a0vedrtdesLRe319Hd+/f196xtF6eXnZe+/NWKdpuh5jXI8xxvn5+djtdn9s2P/NxcXF2Gw2S884ar9+/Tr4B8l+b8Y6z/P9GON+jDHW6/V8dXX12ZuyttvtuLm5WXrGUfv27ds4PT1dekaS76wQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFCLFChFghQqwQIVaIECtEiBUixAoRYoUIsUKEWCFCrBAhVogQK0SIFSLEChFihQixQoRYIUKsECFWiBArRIgVIsQKEWKFiGme58MPTNP1GOP6v4+XY4x/PntU2N9jjOelRxw5Z3TY5TzPp7+78WasvN80TY/zPK+X3nHMnNFhh87HazBEiBUixPpn3S89IMAZHbb3fHxnhQj/WSFCrBAhVogQK0SIFSL+BagGRIe7+P5ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "4113a2cb-cd8b-4fa5-b17f-099e1ea2a608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(total_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396c15c-797f-415c-aed2-84a5e08b9d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4e73d-c4de-4319-a60c-77701e9feb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb4721-0906-4420-8fb8-f822ccdf75bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26520366-6ac4-4036-8eef-50abf596b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49883e9-8a02-4510-a4cf-82bac6e72844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588b97e-f573-4422-bf76-1669a9fc6e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "546e915e-cb91-42ba-b1ca-7bbd76b392ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "records = []  # state, action, reward\n",
    "\n",
    "# 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "while qmaze.game_status() == 'not_over':\n",
    "    # 初次观测\n",
    "    rat_row, rat_col, mode = qmaze.state\n",
    "    # 选择动作\n",
    "    # print(rat_row, rat_col, mode)\n",
    "    action = choose_action((rat_row, rat_col), epsilon)\n",
    "    \n",
    "    state, action = state_to_index((rat_row, rat_col)), action\n",
    "    \n",
    "    # 执行动作并二次观测\n",
    "    canvas, reward, game_over = qmaze.act(action)\n",
    "    \n",
    "    records.append((state, action, reward))\n",
    "    \n",
    "    rat_row_next, rat_col_next, mode = qmaze.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "46bf7fbb-5653-4a8b-8fbc-4009867e9f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, -0.75), (0, 3, -0.04), (3, 2, -0.04), (4, 3, -0.04), (7, 2, 1.0)]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "ab0021fb-3597-4845-a8a0-a728af226227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7812117768267714106"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "de51dee2-a682-4061-8111-62c28d633605",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qmaze' object has no attribute 'canvas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [488]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mqmaze\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Qmaze' object has no attribute 'canvas'"
     ]
    }
   ],
   "source": [
    "qmaze.canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b87e6b-1ecb-492b-bdd7-17a52d7db9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def run_maze():\n",
    "    env = MazeEnv(maze_size=3)\n",
    "    num_episodes = 10000\n",
    "    Q = np.zeros((env.maze_size, env.maze_size, 4))  # 初始化状态-动作价值函数\n",
    "    N = np.zeros((env.maze_size, env.maze_size, 4))  # 记录每个状态-动作被采样的次数\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "\n",
    "        # 进行一次完整的 episode，并记录每个状态-动作的信息\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_action(state, Q)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # 使用经验采样更新状态-动作价值函数\n",
    "        G = 0  # 记录返回值\n",
    "        for j in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[j]\n",
    "            G += reward\n",
    "            N[state][action] += 1\n",
    "            alpha = 1 / N[state][action]\n",
    "            Q[state][action] += alpha * (G - Q[state][action])\n",
    "\n",
    "    # 输出最终的状态-动作价值函数\n",
    "    print(\"Final Q:\")\n",
    "    print(Q)\n",
    "\n",
    "def choose_action(state, Q, eps=0.1):\n",
    "    if random.random() < eps:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b9c8c-131c-4d5d-83d4-5fc6d12ee5ef",
   "metadata": {},
   "source": [
    "## backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39c1a9af-2342-4678-9629-eb7161d4363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "        \n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "502c7a8b-826b-46e1-a496-66da8ccac60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfb620-aaf8-4260-b21a-0c535dc7607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cc97c-1210-474c-ac29-c96badbceb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83fa20-7630-4b69-875d-97f3c98fe2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f52e3-8241-421f-b67e-ec4b76421a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
